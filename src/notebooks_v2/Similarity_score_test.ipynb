{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bert_score import score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saideepbunny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([\"overqualified\", \"underqualified\", \"mismatch\", \"good\"])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by removing unwanted symbols, normalizing, and removing stopwords.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s%$/.-]\", \"\", text)\n",
    "    text = re.sub(r\"-(?!\\d)\", \"\", text)  # Preserve hyphens only when followed by a number\n",
    "    text = re.sub(r\"(?<!\\d)/|/(?!\\d)\", \" \", text)  # Preserve GPA-like formats (e.g., 3.8/4.0)\n",
    "    text = re.sub(r\"\\b(\\w+)\\.(?!\\d)\", r\"\\1\", text)  # Remove periods unless in numbers\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = text.replace(\"show less\", \"\").replace(\"show more\", \"\")\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "def chunk_text(text, max_length=510, overlap=50):\n",
    "    \"\"\"Splits long text into overlapping chunks to fit the model's context limit.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_length - overlap):\n",
    "        chunk = \" \".join(words[i : i + max_length])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"Generates an aggregated embedding for long text using chunking and mean pooling.\"\"\"\n",
    "    chunks = chunk_text(text)\n",
    "    chunk_embeddings = model.encode(chunks)  # Get embeddings for each chunk\n",
    "    \n",
    "    if len(chunk_embeddings) == 0:\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())  # Return zero vector if no embeddings\n",
    "    \n",
    "    # Mean pooling to aggregate chunk embeddings into a single vector\n",
    "    final_embedding = np.mean(chunk_embeddings, axis=0)  \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Compute BERTScore (Semantic Similarity)\n",
    "def compute_bertscore(candidate, reference):\n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\", model_type=\"roberta-base\")\n",
    "    return [P.item(), R.item(), F1.item()]  # Use F1 score for evaluation\n",
    "\n",
    "\n",
    "# 2. Compute Cosine Similarity (Lexical Similarity using TF-IDF)\n",
    "def compute_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "\n",
    "\n",
    "# 3. Compute Jaccard Similarity (Word Overlap Measure)\n",
    "def compute_jaccard_similarity(text1, text2):\n",
    "    words1 = set(word_tokenize(text1.lower())) - stop_words\n",
    "    words2 = set(word_tokenize(text2.lower())) - stop_words\n",
    "    intersection = words1.intersection(words2)\n",
    "    union = words1.union(words2)\n",
    "    return len(intersection) / len(union) if union else 0\n",
    "\n",
    "# 4. Compute cosine Similarity (with sentence-transformers embeddings)\n",
    "def compare_job_resume(resume_text, job_text):\n",
    "    \"\"\"Computes similarity score between a job description and a resume.\"\"\"\n",
    "    job_embedding = get_text_embedding(job_text)\n",
    "    resume_embedding = get_text_embedding(resume_text)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity_score = cosine_similarity([job_embedding], [resume_embedding])[0][0]\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining simple job descriptions and resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job Description & Resumes\n",
    "job_description = \"\"\"We’re looking for a Data Scientist to develop and deploy machine learning models that drive business insights and operational efficiency. You’ll collaborate with cross-functional teams to analyze complex data, optimize AI-driven applications, and extract actionable intelligence.\n",
    "\n",
    "What You’ll Do:\n",
    "\n",
    "Develop, train, and deploy machine learning models.\n",
    "Analyze and interpret large datasets to uncover insights.\n",
    "Optimize model performance and application efficiency.\n",
    "Collaborate with subject matter experts to validate data-driven decisions.\n",
    "Document findings, best practices, and technical workflows.\n",
    "\n",
    "\n",
    "Preferred Qualifications:\n",
    "\n",
    "MS/PhD in Computer Science, Statistics, or a related field.\n",
    "Strong background in machine learning, regression, and classification.\n",
    "Proficiency in Python, SQL, and data visualization tools.\n",
    "Experience with time-series analysis and scalable ML solutions.\n",
    "Excellent analytical, problem-solving, and communication skills.\"\"\"\n",
    "\n",
    "\n",
    "resumes = {\n",
    "    # 1. Complete Mismatch  \n",
    "    \"Complete Mismatch\" : \"\"\"John Doe  \n",
    "    123 Main St, City, State, 12345  \n",
    "    Email: johndoe@example.com | Phone: (123) 456-7890  \n",
    "\n",
    "    Objective: Seeking a role as a Graphic Designer where I can utilize my creativity and design skills.  \n",
    "\n",
    "    Experience:  \n",
    "    - Freelance Graphic Designer (2018-Present)  \n",
    "      - Created marketing materials for small businesses.  \n",
    "      - Designed logos, brochures, and social media assets.  \n",
    "\n",
    "    Skills:  \n",
    "    - Adobe Photoshop, Illustrator, InDesign  \n",
    "    - Branding and visual storytelling  \n",
    "    - Social media marketing  \n",
    "\n",
    "    Education:  \n",
    "    - B.A. in Fine Arts, University of XYZ (2016)\"\"\",\n",
    "\n",
    "    # 2. Underwhelming Candidate  \n",
    "    \"Underwhelming\" : \"\"\"Jane Smith  \n",
    "    456 Elm St, City, State, 12345  \n",
    "    Email: janesmith@example.com | Phone: (234) 567-8901  \n",
    "\n",
    "    Objective: Entry-level data analyst eager to apply basic data analysis skills in a professional setting.  \n",
    "\n",
    "    Experience:  \n",
    "    - Data Entry Clerk, ABC Corp (2022-Present)  \n",
    "      - Input and processed customer data in Excel.  \n",
    "      - Assisted in generating basic reports.  \n",
    "\n",
    "    Skills:  \n",
    "    - Excel, Basic SQL  \n",
    "    - Basic Python (Pandas, NumPy)  \n",
    "    - Data cleaning and entry  \n",
    "\n",
    "    Education:  \n",
    "    - B.A. in Business Administration, University of ABC (2021)\"\"\",\n",
    "\n",
    "    # 3. Good Fit Candidate  \n",
    "    \"Good fit\" : \"\"\"Michael Johnson  \n",
    "    789 Oak St, City, State, 12345  \n",
    "    Email: michaeljohnson@example.com | Phone: (345) 678-9012  \n",
    "\n",
    "    Objective: Data Scientist with a passion for leveraging machine learning to drive business insights and operational efficiency.  \n",
    "\n",
    "    Experience:  \n",
    "    - Data Scientist, XYZ Tech (2021-Present)  \n",
    "      - Developed and deployed ML models for customer segmentation and fraud detection.  \n",
    "      - Optimized machine learning pipelines for large-scale data processing.  \n",
    "      - Collaborated with cross-functional teams to integrate AI-driven solutions.  \n",
    "\n",
    "    Skills:  \n",
    "    - Python (Scikit-learn, TensorFlow, Pandas)  \n",
    "    - SQL, Data Visualization (Tableau, Matplotlib)  \n",
    "    - Time-Series Analysis, Classification, Regression  \n",
    "\n",
    "    Education:  \n",
    "    - M.S. in Data Science, University of DEF (2020)\"\"\",\n",
    "\n",
    "    # 4. Overqualified Candidate  \n",
    "    \"Overqualified\" : \"\"\"Dr. Emily Carter  \n",
    "    321 Maple St, City, State, 12345  \n",
    "    Email: emilycarter@example.com | Phone: (456) 789-0123  \n",
    "\n",
    "    Objective: AI/ML researcher seeking to lead high-impact projects in advanced machine learning and deep learning applications.  \n",
    "\n",
    "    Experience:  \n",
    "    - Principal Data Scientist, Global AI Labs (2018-Present)  \n",
    "      - Designed and implemented cutting-edge deep learning models for autonomous systems.  \n",
    "      - Led a team of data scientists and engineers in developing scalable AI solutions.  \n",
    "      - Published multiple papers in top-tier AI conferences.  \n",
    "\n",
    "    Skills:  \n",
    "    - Deep Learning (Transformer models, GANs)  \n",
    "    - Big Data (Spark, Hadoop), Cloud ML Deployment (AWS, GCP)  \n",
    "    - Advanced Statistical Modeling, Bayesian Inference  \n",
    "\n",
    "    Education:  \n",
    "    - Ph.D. in Machine Learning, MIT (2015)\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume fit category:  Complete Mismatch\n",
      "- Bert Scores:  [0.7952219247817993, 0.8138419389724731, 0.8044242262840271]\n",
      "- Cosine Similarity with tfidf:  0.01647419402894092\n",
      "- Cosine Similarity with sentence embeddings:  0.37291026\n",
      "- Jaccard Similarity:  0.01652892561983471\n",
      "\n",
      "***************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume fit category:  Underwhelming\n",
      "- Bert Scores:  [0.7990970611572266, 0.8183722496032715, 0.8086197972297668]\n",
      "- Cosine Similarity with tfidf:  0.11258922797674731\n",
      "- Cosine Similarity with sentence embeddings:  0.6030619\n",
      "- Jaccard Similarity:  0.06086956521739131\n",
      "\n",
      "***************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume fit category:  Good fit\n",
      "- Bert Scores:  [0.8329084515571594, 0.8659994602203369, 0.84913170337677]\n",
      "- Cosine Similarity with tfidf:  0.3316573301657758\n",
      "- Cosine Similarity with sentence embeddings:  0.6797707\n",
      "- Jaccard Similarity:  0.22807017543859648\n",
      "\n",
      "***************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume fit category:  Overqualified\n",
      "- Bert Scores:  [0.8102753162384033, 0.8361140489578247, 0.8229919672012329]\n",
      "- Cosine Similarity with tfidf:  0.20017990502141964\n",
      "- Cosine Similarity with sentence embeddings:  0.60573125\n",
      "- Jaccard Similarity:  0.09090909090909091\n",
      "\n",
      "***************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute Similarity Scores\n",
    "scores = []\n",
    "for fit in resumes.keys():\n",
    "    resume_data = preprocess_text(resumes[fit])\n",
    "    job_data = preprocess_text(job_description)\n",
    "\n",
    "    bert_scores = compute_bertscore(resume_data, job_data)\n",
    "    cosine_score = compute_cosine_similarity(resume_data, job_data)\n",
    "    cosine_score_with_embeddings = compare_job_resume(resume_data, job_data)\n",
    "    jaccard_score = compute_jaccard_similarity(resume_data, job_data)\n",
    "\n",
    "\n",
    "    print(\"Resume fit category: \", fit)\n",
    "    print(\"- Bert Scores: \", bert_scores)\n",
    "    print(\"- Cosine Similarity with tfidf: \", cosine_score)\n",
    "    print(\"- Cosine Similarity with sentence embeddings: \", cosine_score_with_embeddings)\n",
    "    print(\"- Jaccard Similarity: \", jaccard_score)\n",
    "\n",
    "    print(\"\\n***************************************************************************\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
