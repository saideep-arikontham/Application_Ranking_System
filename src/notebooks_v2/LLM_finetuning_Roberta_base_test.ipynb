{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5938511e9ca304d0",
   "metadata": {
    "collapsed": false,
    "id": "5938511e9ca304d0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# LoRA Fine-tuning : Roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aUTa2ZdJexs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3aUTa2ZdJexs",
    "outputId": "10f29c94-65dd-45d6-ac0f-bc40f679b088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9bc98d6413f73",
   "metadata": {
    "collapsed": false,
    "id": "c6c9bc98d6413f73",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T02:07:15.850624Z",
     "start_time": "2025-03-12T02:07:08.713644Z"
    },
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig, DataCollatorWithPadding\n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import huggingface_hub\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from collections import Counter\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5120572f76dc6ace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T02:07:15.853183Z",
     "start_time": "2025-03-12T02:07:15.851575Z"
    },
    "id": "5120572f76dc6ace"
   },
   "outputs": [],
   "source": [
    "path = \"/Users/saideepbunny/Projects/Application_Ranking_System\"\n",
    "huggingface_hub.login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2333ef71af2333d",
   "metadata": {
    "collapsed": false,
    "id": "c2333ef71af2333d",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "601ff9ce3279e9c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T02:07:18.906326Z",
     "start_time": "2025-03-12T02:07:15.853703Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "601ff9ce3279e9c2",
    "outputId": "e75a4b92-e64d-4b53-cdab-37b4215c9505"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['job_data', 'resume_data', 'label', '__index_level_0__'],\n",
       "        num_rows: 3200\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['job_data', 'resume_data', 'label', '__index_level_0__'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['job_data', 'resume_data', 'label', '__index_level_0__'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"saideep-arikontham/jd_resume_dataset\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26be598b9db7348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T15:52:44.642735Z",
     "start_time": "2025-03-11T15:52:44.635673Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "26be598b9db7348",
    "outputId": "18077b3e-4cf3-40e6-fcb1-1e7b0c4cf70e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "label\n",
       "0    1600\n",
       "1    1600\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = dataset['train'].to_pandas()\n",
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0862c0-1254-4b57-b2a7-43ad9f588add",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "ca0862c0-1254-4b57-b2a7-43ad9f588add",
    "outputId": "9976f628-a546-49fe-9fb9-6b5263c1def5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "label\n",
       "0    200\n",
       "1    200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = dataset['validation'].to_pandas()\n",
    "val_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84823c54-d7f9-4231-a8e1-e34c193fdd5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "84823c54-d7f9-4231-a8e1-e34c193fdd5e",
    "outputId": "a1e8940c-ff83-4575-a5a7-98346b86286e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "label\n",
       "0    200\n",
       "1    200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = dataset['test'].to_pandas()\n",
    "test_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ecdc1-43c6-4ec9-879a-7b7995150b29",
   "metadata": {
    "id": "683ecdc1-43c6-4ec9-879a-7b7995150b29"
   },
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f315d3-ef5e-49c6-8188-7fe6c0d5d878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T15:52:47.426354Z",
     "start_time": "2025-03-11T15:52:44.958560Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4f315d3-ef5e-49c6-8188-7fe6c0d5d878",
    "outputId": "ab7ce0e3-8915-4078-aa40-f6ac9400005d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Load tokenizer and base model\n",
    "# -------------------------------\n",
    "\n",
    "model_checkpoint = 'roberta-base'\n",
    "\n",
    "# define label maps\n",
    "label2id = {'Bad Fit': 0, 'Good Fit': 1}\n",
    "id2label = {0:'Bad Fit', 1:'Good Fit'}\n",
    "\n",
    "# generate classification model from model_checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dpf5Z1hJjnz3",
   "metadata": {
    "id": "dpf5Z1hJjnz3"
   },
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e537d-a685-4a4d-a0fb-3f6082e5deb8",
   "metadata": {
    "id": "9d4e537d-a685-4a4d-a0fb-3f6082e5deb8"
   },
   "source": [
    "## Define Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "818e8cac-3fab-4837-8d9b-f63dfcb24314",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344,
     "referenced_widgets": [
      "04ca3b8b3cc54081b4f3612b9a6b3133",
      "eab2d2fe91064cbdbe692ee14bc92a78",
      "ebe59cf241f642ccaf909d2e4fbc699e",
      "5be9f0defc874a21b59da304a98b7b18",
      "617e831617c34a518c6d56605d79273f",
      "0edfd44c70324b37aa77156374b1d02f",
      "d5cb764f0db14b6d80e6a81f442cd8f0",
      "0bbe5222f2df4423b11eaf1f669c8edd",
      "822942a13ba34006af76a507a6114d01",
      "168958294b704c7f912b5eb69aa3991d",
      "fb0cb066be484da38857f16675de2c5d"
     ]
    },
    "id": "818e8cac-3fab-4837-8d9b-f63dfcb24314",
    "outputId": "f6c5816e-76fc-49a3-8e3c-29d87f5fc7c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ca3b8b3cc54081b4f3612b9a6b3133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (771 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 19421\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2375\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2366\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Improved tokenization approach for job and resume matching\n",
    "# -------------------------------\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"overqualified\")\n",
    "stop_words.add(\"underqualified\")\n",
    "stop_words.add(\"mismatch\")\n",
    "stop_words.add(\"good\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove unwanted symbols except %, $, /, and .\n",
    "    text = re.sub(r\"[^a-z0-9\\s%$/.-]\", \"\", text)\n",
    "\n",
    "    # Preserve hyphens only when followed by a number (e.g., 2005-2010, 2010-present)\n",
    "    text = re.sub(r\"-(?!\\d)\", \"\", text)  # Remove hyphens not followed by a digit\n",
    "\n",
    "    # Preserve GPA-like formats (e.g., 3.8/4.0)\n",
    "    text = re.sub(r\"(?<!\\d)/|/(?!\\d)\", \" \", text)  # Remove '/' unless between numbers\n",
    "\n",
    "    # Remove periods (\".\") if they are immediately after a word but not numbers (e.g., \"good.\" → \"good\", but keep 3.8)\n",
    "    text = re.sub(r\"\\b(\\w+)\\.(?!\\d)\", r\"\\1\", text)\n",
    "\n",
    "    # Remove newline characters\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "    # Remove any show less and show more texts\n",
    "    text = text.replace(\"show less\", \"\").replace(\"show more\", \"\")\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def chunk_dataset(dataset, tokenizer):\n",
    "    \"\"\"Tokenizes and chunks job and resume data into fixed-size chunks.\"\"\"\n",
    "\n",
    "    job_chunk_size = 256\n",
    "    resume_chunk_size = 256\n",
    "    max_length = job_chunk_size + resume_chunk_size\n",
    "\n",
    "\n",
    "    def chunk_function(examples):\n",
    "        all_input_ids = []  # Store flattened list of input_ids\n",
    "        all_attention_masks = []  # Store flattened list of attention_masks\n",
    "        all_labels = []  # Store flattened list of labels\n",
    "\n",
    "        examples['job_data'] = [preprocess_text(jd) for jd in examples['job_data']]\n",
    "        examples['resume_data'] = [preprocess_text(jd) for jd in examples['resume_data']]\n",
    "\n",
    "        for job_text, resume_text, label in zip(examples['job_data'], examples['resume_data'], examples['label']):\n",
    "            # Tokenize job data\n",
    "            job_tokens = tokenizer.tokenize(job_text)\n",
    "            # Tokenize resume data\n",
    "            resume_tokens = tokenizer.tokenize(resume_text)\n",
    "\n",
    "            num_job_chunks = (len(job_tokens) + job_chunk_size - 1) // job_chunk_size\n",
    "            num_resume_chunks = (len(resume_tokens) + resume_chunk_size - 1) // resume_chunk_size\n",
    "\n",
    "            # Chunk job data\n",
    "            for i in range(0, len(job_tokens), job_chunk_size):\n",
    "                job_chunk = job_tokens[i:i + job_chunk_size]\n",
    "\n",
    "                # Chunk resume data\n",
    "                for j in range(0, len(resume_tokens), resume_chunk_size):\n",
    "                    resume_chunk = resume_tokens[j:j + resume_chunk_size]\n",
    "\n",
    "                    # Combine the chunks and truncate\n",
    "                    combined_tokens = ['[CLS]'] + job_chunk + ['[SEP]'] + resume_chunk\n",
    "                    combined_tokens = combined_tokens[:max_length]\n",
    "\n",
    "                    # Convert to input IDs and attention mask\n",
    "                    input_ids = tokenizer.convert_tokens_to_ids(combined_tokens)\n",
    "                    attention_mask = [1] * len(input_ids)  # 1 for real tokens, 0 for padding\n",
    "\n",
    "                    # Pad to max_length\n",
    "                    padding_length = max_length - len(input_ids)\n",
    "                    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "                    attention_mask = attention_mask + [0] * padding_length\n",
    "\n",
    "                    all_input_ids.append(input_ids) # Append to the flattened list\n",
    "                    all_attention_masks.append(attention_mask) # Append to the flattened list\n",
    "                    all_labels.append(label) # Assign same label to all chunks, append to flattened list\n",
    "\n",
    "        #print(all_input_ids)  # Now print the flattened input_ids\n",
    "        return {\n",
    "            'input_ids': all_input_ids,\n",
    "            'attention_mask': all_attention_masks,\n",
    "            'labels': all_labels\n",
    "        }\n",
    "\n",
    "    # Apply chunking to your dataset\n",
    "    dataset = dataset.map(chunk_function, batched=True, remove_columns=dataset.column_names) #remove original columns\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "tokenized_dataset = DatasetDict()\n",
    "for split in dataset.keys():\n",
    "    tokenized_dataset[split] = chunk_dataset(dataset[split], tokenizer)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mSHJcdXej_J_",
   "metadata": {
    "id": "mSHJcdXej_J_"
   },
   "outputs": [],
   "source": [
    "# create data collator\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m2lX8ZY3-p0y",
   "metadata": {
    "id": "m2lX8ZY3-p0y"
   },
   "source": [
    "## Testing Untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3BMO-UCH2fka",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BMO-UCH2fka",
    "outputId": "81459772-6e35-4fd0-97b4-5724663fb45d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([\"overqualified\", \"underqualified\", \"mismatch\", \"good\"])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by removing unwanted symbols, normalizing, and removing stopwords.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s%$/.-]\", \"\", text)\n",
    "    text = re.sub(r\"-(?!\\d)\", \"\", text)  # Preserve hyphens only when followed by a number\n",
    "    text = re.sub(r\"(?<!\\d)/|/(?!\\d)\", \" \", text)  # Preserve GPA-like formats (e.g., 3.8/4.0)\n",
    "    text = re.sub(r\"\\b(\\w+)\\.(?!\\d)\", r\"\\1\", text)  # Remove periods unless in numbers\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = text.replace(\"show less\", \"\").replace(\"show more\", \"\")\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "qk01jZbJ9W6T",
   "metadata": {
    "id": "qk01jZbJ9W6T"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def predict_with_voting(job_text, resume_text, model, tokenizer, device=\"cpu\", return_probabilities=True):\n",
    "    \"\"\"\n",
    "    Predicts the class using chunking and a voting mechanism.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        job_text = preprocess_text(job_text)\n",
    "        resume_text = preprocess_text(resume_text)\n",
    "\n",
    "        # Tokenize job data\n",
    "        job_tokens = tokenizer.tokenize(job_text)\n",
    "        # Tokenize resume data\n",
    "        resume_tokens = tokenizer.tokenize(resume_text)\n",
    "\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "\n",
    "        # Chunk job data\n",
    "        for i in range(0, len(job_tokens), job_chunk_size):\n",
    "            job_chunk = job_tokens[i:i + job_chunk_size]\n",
    "\n",
    "            # Chunk resume data\n",
    "            for j in range(0, len(resume_tokens), resume_chunk_size):\n",
    "                resume_chunk = resume_tokens[j:j + resume_chunk_size]\n",
    "\n",
    "                # Combine the chunks and truncate\n",
    "                combined_tokens = ['[CLS]'] + job_chunk + ['[SEP]'] + resume_chunk\n",
    "                combined_tokens = combined_tokens[:max_length]\n",
    "\n",
    "                # Convert to input IDs and attention mask\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(combined_tokens)\n",
    "                attention_mask = [1] * len(input_ids)  # 1 for real tokens, 0 for padding\n",
    "\n",
    "                # Pad to max_length\n",
    "                padding_length = max_length - len(input_ids)\n",
    "                input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "                attention_mask = attention_mask + [0] * padding_length\n",
    "\n",
    "                # Convert to tensors and send to device\n",
    "                input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "                attention_mask = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs if isinstance(outputs, torch.Tensor) else outputs.logits\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "                predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "                predictions.append(predicted_class)\n",
    "                probabilities.append(probs[0, 1].item())  # Probability of class \"1\"\n",
    "\n",
    "        # Hard Voting\n",
    "        if predictions:\n",
    "            counts = Counter(predictions)\n",
    "            most_voted_class = counts.most_common(1)[0][0]\n",
    "        else:\n",
    "            most_voted_class = 0  # Default if no predictions\n",
    "\n",
    "        # Average Probability for Class \"1\"\n",
    "        average_probability = np.mean(probabilities) if probabilities else 0.0\n",
    "\n",
    "    return most_voted_class, average_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3EJ1lDsjDTTP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3EJ1lDsjDTTP",
    "outputId": "844b0d94-93a6-42ea-f319-df42d0545306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 15 15:43:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   68C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aXeIE_c9YmV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aXeIE_c9YmV",
    "outputId": "a26f3e9c-1048-4893-b1c1-83546dd22ba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Accuracy:  0.5\n",
      "Precision:  0.5\n",
      "Recall:  1.0\n",
      "F1:  0.6666666666666666\n",
      "Confusion Matrix:  [[  0 200]\n",
      " [  0 200]]\n"
     ]
    }
   ],
   "source": [
    "# Model and Tokenizer Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #changed to torch.device\n",
    "print(device)\n",
    "\n",
    "# Define chunk sizes\n",
    "job_chunk_size = 256\n",
    "resume_chunk_size = 256\n",
    "max_length = job_chunk_size + resume_chunk_size\n",
    "\n",
    "# Load test data\n",
    "y_test = test_df['label'].tolist()\n",
    "y_pred = []\n",
    "\n",
    "# Run Predictions\n",
    "for i in range(test_df.shape[0]):\n",
    "    jd = test_df.iloc[i]['job_data']\n",
    "    rd = test_df.iloc[i]['resume_data']\n",
    "\n",
    "    predicted_class, class_1_prob = predict_with_voting(jd, rd, model, tokenizer, device)\n",
    "    y_pred.append(predicted_class)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred))\n",
    "print(\"F1: \", f1_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee92f2-6aaf-4295-ae58-b18d29b318aa",
   "metadata": {
    "id": "85ee92f2-6aaf-4295-ae58-b18d29b318aa"
   },
   "source": [
    "## Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7tLyu34jkKUC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tLyu34jkKUC",
    "outputId": "31b152d6-9a91-4743-90a4-ea0539030329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 125,534,212 || trainable%: 0.7066\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Configure LoRA fine-tuning\n",
    "# -------------------------------\n",
    "\n",
    "# Define a LoRA configuration. Adjust parameters (r, lora_alpha, etc.) as needed.\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # for sequence classification\n",
    "    r=8,                        # low rank parameter; experiment with this value\n",
    "    lora_alpha=32,              # scaling parameter\n",
    "    lora_dropout=0.15,           # dropout probability for LoRA layers\n",
    "    target_modules=[\"query\", \"value\"]  # adjust based on your model architecture\n",
    ")\n",
    "\n",
    "# Wrap your model with LoRA. This freezes most of the model and inserts trainable LoRA layers.\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368b17d-c46d-4fdd-a5b7-be9ba846496e",
   "metadata": {
    "id": "2368b17d-c46d-4fdd-a5b7-be9ba846496e"
   },
   "source": [
    "## Setting Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff53a976-660c-47dd-9dc9-f54c3d3e63d9",
   "metadata": {
    "id": "ff53a976-660c-47dd-9dc9-f54c3d3e63d9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Setup training parameters\n",
    "# -------------------------------\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",             # Set save strategy to epoch to match evaluation_strategy\n",
    "    num_train_epochs=1,                # Adjust number of epochs as desired\n",
    "    per_device_train_batch_size=32,    # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    load_best_model_at_end=True,       # Load the best model when finished training (if metric provided)\n",
    "    metric_for_best_model=\"accuracy\",  # Choose your metric\n",
    "    weight_decay=0.1,                  # Strong L2 Regularization (Higher Regularization)\n",
    "    max_grad_norm=0.5,                  # Aggressive Gradient Clipping\n",
    "    adam_beta1=0.9,                      # Standard Momentum\n",
    "    adam_beta2=0.98,                     # Reduces dependency on past gradients\n",
    "    adam_epsilon=1e-08,                   # Prevents division by zero\n",
    "    label_smoothing_factor=0.1,         # Helps prevent overconfidence\n",
    "    warmup_ratio=0.1                   # 10% of training steps as warm-up\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b03d4-bac6-4dca-86b9-d8efc29aa4f4",
   "metadata": {
    "id": "588b03d4-bac6-4dca-86b9-d8efc29aa4f4"
   },
   "source": [
    "## Defining Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24d031c5-c398-4b0e-9330-be4f2e890cc3",
   "metadata": {
    "id": "24d031c5-c398-4b0e-9330-be4f2e890cc3"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Define a metric function for evaluation\n",
    "# -------------------------------\n",
    "\n",
    "# Load metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d0b06-915f-4a42-ade2-0d8dbeffd8c2",
   "metadata": {
    "id": "556d0b06-915f-4a42-ade2-0d8dbeffd8c2"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e42cac61-d23e-4ea3-9db1-9d7fa395b144",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "e42cac61-d23e-4ea3-9db1-9d7fa395b144",
    "outputId": "2ef30256-eef7-4b97-d794-244a5c14fdf3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaideepreddy99\u001b[0m (\u001b[33msaideepreddy99-northeastern-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250315_154603-pmhx2e1q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/saideepreddy99-northeastern-university/huggingface/runs/pmhx2e1q' target=\"_blank\">test1</a></strong> to <a href='https://wandb.ai/saideepreddy99-northeastern-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/saideepreddy99-northeastern-university/huggingface' target=\"_blank\">https://wandb.ai/saideepreddy99-northeastern-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/saideepreddy99-northeastern-university/huggingface/runs/pmhx2e1q' target=\"_blank\">https://wandb.ai/saideepreddy99-northeastern-university/huggingface/runs/pmhx2e1q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='607' max='607' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [607/607 24:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>0.379908</td>\n",
       "      <td>0.884632</td>\n",
       "      <td>0.885116</td>\n",
       "      <td>0.884632</td>\n",
       "      <td>0.884376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=607, training_loss=0.5239724311640078, metrics={'train_runtime': 1555.7338, 'train_samples_per_second': 12.483, 'train_steps_per_second': 0.39, 'total_flos': 5162801895665664.0, 'train_loss': 0.5239724311640078, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Create the Trainer and start training\n",
    "# -------------------------------\n",
    "\n",
    "# creater trainer object\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c85e31-bd7e-4086-abaf-2c70034c56f7",
   "metadata": {
    "id": "03c85e31-bd7e-4086-abaf-2c70034c56f7",
    "outputId": "21e18fb8-fc0c-4ce1-9285-891e6418a5b2"
   },
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "980ce2b9-5125-497e-964f-1cac65506d48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "980ce2b9-5125-497e-964f-1cac65506d48",
    "outputId": "2f4f7056-4644-4798-a7d3-5f59d42234c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 01:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4239107668399811,\n",
       " 'eval_accuracy': 0.8655959425190194,\n",
       " 'eval_precision': 0.868844829732984,\n",
       " 'eval_recall': 0.8655959425190194,\n",
       " 'eval_f1': 0.8656727869373435,\n",
       " 'eval_runtime': 68.9357,\n",
       " 'eval_samples_per_second': 34.322,\n",
       " 'eval_steps_per_second': 1.073,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Evaluate the final model on the test set\n",
    "# -------------------------------\n",
    "\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pig-0LhNDF6U",
   "metadata": {
    "id": "pig-0LhNDF6U"
   },
   "source": [
    "## Testing model after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "GzDow3w-DJWX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzDow3w-DJWX",
    "outputId": "1ce550b7-61a2-49c6-ff87-132a5f4e100e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Accuracy:  0.95\n",
      "Precision:  0.9639175257731959\n",
      "Recall:  0.935\n",
      "F1:  0.949238578680203\n",
      "Confusion Matrix:  [[193   7]\n",
      " [ 13 187]]\n"
     ]
    }
   ],
   "source": [
    "# Model and Tokenizer Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #changed to torch.device\n",
    "print(device)\n",
    "\n",
    "# Define chunk sizes\n",
    "job_chunk_size = 256\n",
    "resume_chunk_size = 256\n",
    "max_length = job_chunk_size + resume_chunk_size\n",
    "\n",
    "# Load test data\n",
    "y_test = test_df['label'].tolist()\n",
    "y_pred = []\n",
    "\n",
    "# Run Predictions\n",
    "for i in range(test_df.shape[0]):\n",
    "    jd = test_df.iloc[i]['job_data']\n",
    "    rd = test_df.iloc[i]['resume_data']\n",
    "\n",
    "    predicted_class, class_1_prob = predict_with_voting(jd, rd, lora_model, tokenizer, device)\n",
    "    y_pred.append(predicted_class)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred))\n",
    "print(\"F1: \", f1_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "uH01K1SP9TdY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uH01K1SP9TdY",
    "outputId": "f5835df7-8b48-4fcc-eb15-c30a8a744056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Accuracy:  0.95\n",
      "Precision:  0.9639175257731959\n",
      "Recall:  0.935\n",
      "F1:  0.949238578680203\n",
      "Confusion Matrix:  [[193   7]\n",
      " [ 13 187]]\n"
     ]
    }
   ],
   "source": [
    "# Model and Tokenizer Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #changed to torch.device\n",
    "print(device)\n",
    "\n",
    "# Define chunk sizes\n",
    "job_chunk_size = 256\n",
    "resume_chunk_size = 256\n",
    "max_length = job_chunk_size + resume_chunk_size\n",
    "\n",
    "# Load test data\n",
    "y_test = test_df['label'].tolist()\n",
    "y_pred = []\n",
    "\n",
    "# Run Predictions\n",
    "for i in range(test_df.shape[0]):\n",
    "    jd = test_df.iloc[i]['job_data']\n",
    "    rd = test_df.iloc[i]['resume_data']\n",
    "\n",
    "    predicted_class, class_1_prob = predict_with_voting(jd, rd, model, tokenizer, device)\n",
    "    y_pred.append(predicted_class)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred))\n",
    "print(\"F1: \", f1_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "tA6y1_L-Rxpc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tA6y1_L-Rxpc",
    "outputId": "860b02c7-a4e5-4649-ca6d-5f13c1f7a4a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# prompt: Save the fine-tuned LoRA model to google drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Save the merged model to Google Drive\n",
    "lora_model.save_pretrained(\"/content/drive/MyDrive/lora_adaptors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "l6Soq07KSFW3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "fe04cbaed06f455a9f8fb97155261667",
      "03ada217f6f342b7a4332c8a8ebc7f0a",
      "04023dd9f6a14b159d90691b763280bc",
      "72213bd1d0074e87ae645b8c5142bf50",
      "85f8e4daf9f64c56a44d3d3838e3342e",
      "b6c7ad58898443498343e7d7a56c6f3e",
      "b270aba3a8824a9e86262eb4905b8f6e",
      "d004c0eb09d6444f9f19de28382b214a",
      "6b45daf8b4154eaf9dc16093ff64c3f9",
      "f5a6d2eba4a542d7b503a81311de3ccf",
      "1f8cfe81076144c2b69be185c7807012"
     ]
    },
    "id": "l6Soq07KSFW3",
    "outputId": "44a86b9f-e8ac-4cff-bf9b-74e3af4ddb42"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe04cbaed06f455a9f8fb97155261667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/saideep-arikontham/roberta-resume-fit-predictor-adaptor/commit/41d7f3a0020355858d1fbce2f5455da7a06fb332', commit_message='Upload model', commit_description='', oid='41d7f3a0020355858d1fbce2f5455da7a06fb332', pr_url=None, repo_url=RepoUrl('https://huggingface.co/saideep-arikontham/roberta-resume-fit-predictor-adaptor', endpoint='https://huggingface.co', repo_type='model', repo_id='saideep-arikontham/roberta-resume-fit-predictor-adaptor'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt: Push the fine-tuned model adapters to huggingface\n",
    "\n",
    "# Push the merged model and tokenizer to Hugging Face Hub\n",
    "lora_model.push_to_hub(\"saideep-arikontham/roberta-resume-fit-predictor-adaptor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "plXEQb9XSYmd",
   "metadata": {
    "id": "plXEQb9XSYmd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ARS)",
   "language": "python",
   "name": "ars"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03ada217f6f342b7a4332c8a8ebc7f0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6c7ad58898443498343e7d7a56c6f3e",
      "placeholder": "​",
      "style": "IPY_MODEL_b270aba3a8824a9e86262eb4905b8f6e",
      "value": "adapter_model.safetensors: 100%"
     }
    },
    "04023dd9f6a14b159d90691b763280bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d004c0eb09d6444f9f19de28382b214a",
      "max": 3555504,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6b45daf8b4154eaf9dc16093ff64c3f9",
      "value": 3555504
     }
    },
    "04ca3b8b3cc54081b4f3612b9a6b3133": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eab2d2fe91064cbdbe692ee14bc92a78",
       "IPY_MODEL_ebe59cf241f642ccaf909d2e4fbc699e",
       "IPY_MODEL_5be9f0defc874a21b59da304a98b7b18"
      ],
      "layout": "IPY_MODEL_617e831617c34a518c6d56605d79273f"
     }
    },
    "0bbe5222f2df4423b11eaf1f669c8edd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0edfd44c70324b37aa77156374b1d02f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "168958294b704c7f912b5eb69aa3991d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f8cfe81076144c2b69be185c7807012": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5be9f0defc874a21b59da304a98b7b18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_168958294b704c7f912b5eb69aa3991d",
      "placeholder": "​",
      "style": "IPY_MODEL_fb0cb066be484da38857f16675de2c5d",
      "value": " 400/400 [00:05&lt;00:00, 69.10 examples/s]"
     }
    },
    "617e831617c34a518c6d56605d79273f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b45daf8b4154eaf9dc16093ff64c3f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "72213bd1d0074e87ae645b8c5142bf50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5a6d2eba4a542d7b503a81311de3ccf",
      "placeholder": "​",
      "style": "IPY_MODEL_1f8cfe81076144c2b69be185c7807012",
      "value": " 3.56M/3.56M [00:00&lt;00:00, 6.85MB/s]"
     }
    },
    "822942a13ba34006af76a507a6114d01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "85f8e4daf9f64c56a44d3d3838e3342e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b270aba3a8824a9e86262eb4905b8f6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6c7ad58898443498343e7d7a56c6f3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d004c0eb09d6444f9f19de28382b214a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5cb764f0db14b6d80e6a81f442cd8f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eab2d2fe91064cbdbe692ee14bc92a78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0edfd44c70324b37aa77156374b1d02f",
      "placeholder": "​",
      "style": "IPY_MODEL_d5cb764f0db14b6d80e6a81f442cd8f0",
      "value": "Map: 100%"
     }
    },
    "ebe59cf241f642ccaf909d2e4fbc699e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0bbe5222f2df4423b11eaf1f669c8edd",
      "max": 400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_822942a13ba34006af76a507a6114d01",
      "value": 400
     }
    },
    "f5a6d2eba4a542d7b503a81311de3ccf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb0cb066be484da38857f16675de2c5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe04cbaed06f455a9f8fb97155261667": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03ada217f6f342b7a4332c8a8ebc7f0a",
       "IPY_MODEL_04023dd9f6a14b159d90691b763280bc",
       "IPY_MODEL_72213bd1d0074e87ae645b8c5142bf50"
      ],
      "layout": "IPY_MODEL_85f8e4daf9f64c56a44d3d3838e3342e"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
