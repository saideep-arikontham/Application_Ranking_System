{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Prompts for generating synthetic data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdb71d3ced1c07f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f126534269f0ace"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2133218b-9766-4c8c-ae07-d9a6a4845cf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T23:32:32.045388Z",
     "start_time": "2025-03-08T23:32:30.750847Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Set Pandas to display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "path = \"/Users/saideepbunny/Projects/Application_Ranking_System\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:32:32.047833Z",
     "start_time": "2025-03-08T23:32:32.046333Z"
    }
   },
   "id": "dcab3c15f43d8094",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11167744e1545b5e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaab01d9-3a04-4bad-aa8c-227aecdd26fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T23:32:32.090988Z",
     "start_time": "2025-03-08T23:32:32.048440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              job_link  \\\n0    https://uk.linkedin.com/jobs/view/audit-manage...   \n1    https://www.linkedin.com/jobs/view/audit-manag...   \n2    https://www.linkedin.com/jobs/view/audit-manag...   \n3    https://www.linkedin.com/jobs/view/audit-manag...   \n4    https://uk.linkedin.com/jobs/view/audit-manage...   \n..                                                 ...   \n245  https://www.linkedin.com/jobs/view/test-engine...   \n246  https://www.linkedin.com/jobs/view/test-engine...   \n247  https://www.linkedin.com/jobs/view/test-engine...   \n248  https://www.linkedin.com/jobs/view/test-engine...   \n249  https://www.linkedin.com/jobs/view/test-engine...   \n\n               last_processed_time got_summary got_ner is_being_worked  \\\n0    2024-01-19 09:45:09.215838+00           t       t               f   \n1    2024-01-19 15:33:11.588617+00           t       t               f   \n2    2024-01-19 09:45:09.215838+00           t       t               f   \n3    2024-01-19 09:45:09.215838+00           t       t               f   \n4    2024-01-20 06:56:42.454545+00           t       t               f   \n..                             ...         ...     ...             ...   \n245   2024-01-19 20:05:43.91119+00           t       t               f   \n246  2024-01-19 09:45:09.215838+00           t       t               f   \n247  2024-01-19 09:45:09.215838+00           t       t               f   \n248  2024-01-19 09:45:09.215838+00           t       t               f   \n249  2024-01-19 09:45:09.215838+00           t       t               f   \n\n         job_title                               company  \\\n0    Audit Manager                          ACCA Careers   \n1    Audit Manager                        Baker Tilly US   \n2    Audit Manager                                 KORE1   \n3    Audit Manager                                 Jobot   \n4    Audit Manager  Bishop Fleming Chartered Accountants   \n..             ...                                   ...   \n245  Test Engineer                           Stoke Space   \n246  Test Engineer                               Akkodis   \n247  Test Engineer                     nFocus SolutionsÂ®   \n248  Test Engineer                    Micross Components   \n249  Test Engineer                   SEGULA Technologies   \n\n                           job_location  first_seen     search_city  \\\n0    Stevenage, England, United Kingdom  2024-01-13           Luton   \n1                       Minneapolis, MN  2024-01-15           Orono   \n2                        Cincinnati, OH  2024-01-12      Cincinnati   \n3                          Cheyenne, WY  2024-01-12         Wyoming   \n4      Bristol, England, United Kingdom  2024-01-14         Bristol   \n..                                  ...         ...             ...   \n245                      Moses Lake, WA  2024-01-14         Ephrata   \n246                           Tempe, AZ  2024-01-12          Canyon   \n247                         Orlando, FL  2024-01-13        Avondale   \n248                    Los Alamitos, CA  2024-01-12        Pasadena   \n249                      Greensboro, NC  2024-01-12  North Carolina   \n\n     search_country                 search_position   job_level job_type  \\\n0    United Kingdom                  Director Stage  Mid senior   Onsite   \n1     United States                           Baker  Mid senior   Onsite   \n2     United States                         Auditor  Mid senior   Onsite   \n3     United States                         Auditor  Mid senior   Onsite   \n4    United Kingdom                            Tier  Mid senior   Onsite   \n..              ...                             ...         ...      ...   \n245   United States  Agricultural-Research Engineer  Mid senior   Onsite   \n246   United States  Agricultural-Research Engineer   Associate   Onsite   \n247   United States               Computer Operator  Mid senior   Onsite   \n248   United States  Agricultural-Research Engineer   Associate   Onsite   \n249   United States  Agricultural-Research Engineer   Associate   Onsite   \n\n                                            job_skills  \\\n0    Audit, Financial statements, FRS102, IFRS, Acc...   \n1    Accounting, Auditing, Assurance services, Fina...   \n2    Internal Audit Manager, Riskbased audit progra...   \n3    Audit, Leadership, Client Relationships, Strat...   \n4    Audit, Assurance, Accounting, Business Develop...   \n..                                                 ...   \n245  Fluid systems, Mechanical systems, Instrumenta...   \n246  Aerospace, Mechanical test environment, Mechan...   \n247  Agile Software Development, System Testing, Te...   \n248  Semiconductor Testing, Automated Test Equipmen...   \n249  Data acquisition, Data processing, Mechanical ...   \n\n                                           job_summary      id  summary_len  \\\n0    Menzies LLP\\nWe are looking for an experienced...  339382         6142   \n1    Overview\\nBaker Tilly US, LLP (Baker Tilly) is...   70598         5769   \n2    KORE1, a nationwide provider of staffing and r...   25866         5263   \n3    Want to learn more about this role and Jobot? ...  561971         5066   \n4    Description\\nLocation: Bristol\\nAbout The Role...  885770         4917   \n..                                                 ...     ...          ...   \n245  A thriving economy in space is needed to make ...   14991         4365   \n246  Akkodis is seeking a Test Engineer for a posit...  189702         3999   \n247  Role Summary\\nWork within an Agile Software De...   16993         3581   \n248  Job Summary:\\nPerforms LAT testing, builds bur...  568816         3510   \n249  Company Description\\nMUST be authorized to wor...  452512         3301   \n\n     skill_count  \n0             25  \n1             26  \n2             51  \n3             30  \n4             32  \n..           ...  \n245           15  \n246           33  \n247           28  \n248           40  \n249           16  \n\n[500 rows x 19 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>job_link</th>\n      <th>last_processed_time</th>\n      <th>got_summary</th>\n      <th>got_ner</th>\n      <th>is_being_worked</th>\n      <th>job_title</th>\n      <th>company</th>\n      <th>job_location</th>\n      <th>first_seen</th>\n      <th>search_city</th>\n      <th>search_country</th>\n      <th>search_position</th>\n      <th>job_level</th>\n      <th>job_type</th>\n      <th>job_skills</th>\n      <th>job_summary</th>\n      <th>id</th>\n      <th>summary_len</th>\n      <th>skill_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://uk.linkedin.com/jobs/view/audit-manage...</td>\n      <td>2024-01-19 09:45:09.215838+00</td>\n      <td>t</td>\n      <td>t</td>\n      <td>f</td>\n      <td>Audit Manager</td>\n      <td>ACCA Careers</td>\n      <td>Stevenage, England, United Kingdom</td>\n      <td>2024-01-13</td>\n      <td>Luton</td>\n      <td>United Kingdom</td>\n      <td>Director Stage</td>\n      <td>Mid senior</td>\n      <td>Onsite</td>\n      <td>Audit, Financial statements, FRS102, IFRS, Acc...</td>\n      <td>Menzies LLP\\nWe are looking for an experienced...</td>\n      <td>339382</td>\n      <td>6142</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.linkedin.com/jobs/view/audit-manag...</td>\n      <td>2024-01-19 15:33:11.588617+00</td>\n      <td>t</td>\n      <td>t</td>\n      <td>f</td>\n      <td>Audit Manager</td>\n      <td>Baker Tilly US</td>\n      <td>Minneapolis, MN</td>\n      <td>2024-01-15</td>\n      <td>Orono</td>\n      <td>United States</td>\n      <td>Baker</td>\n      <td>Mid senior</td>\n      <td>Onsite</td>\n      <td>Accounting, Auditing, Assurance services, Fina...</td>\n      <td>Overview\\nBaker Tilly US, LLP (Baker Tilly) is...</td>\n      <td>70598</td>\n      <td>5769</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.linkedin.com/jobs/view/audit-manag...</td>\n      <td>2024-01-19 09:45:09.215838+00</td>\n      <td>t</td>\n      <td>t</td>\n      <td>f</td>\n      <td>Audit Manager</td>\n      <td>KORE1</td>\n      <td>Cincinnati, OH</td>\n      <td>2024-01-12</td>\n      <td>Cincinnati</td>\n      <td>United States</td>\n      <td>Auditor</td>\n      <td>Mid senior</td>\n      <td>Onsite</td>\n      <td>Internal Audit Manager, Riskbased audit progra...</td>\n      <td>KORE1, a nationwide provider of staffing and r...</td>\n      <td>25866</td>\n      <td>5263</td>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.linkedin.com/jobs/view/audit-manag...</td>\n      <td>2024-01-19 09:45:09.215838+00</td>\n      <td>t</td>\n      <td>t</td>\n      <td>f</td>\n      <td>Audit Manager</td>\n      <td>Jobot</td>\n      <td>Cheyenne, WY</td>\n      <td>2024-01-12</td>\n      <td>Wyoming</td>\n      <td>United States</td>\n      <td>Auditor</td>\n      <td>Mid senior</td>\n      <td>Onsite</td>\n      <td>Audit, Leadership, Client Relationships, Strat...</td>\n      <td>Want to learn more about this role and Jobot? ...</td>\n      <td>561971</td>\n      <td>5066</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://uk.linkedin.com/jobs/view/audit-manage...</td>\n      <td>2024-01-20 06:56:42.454545+00</td>\n      <td>t</td>\n      <td>t</td>\n      <td>f</td>\n      <td>Audit Manager</td>\n      <td>Bishop Fleming Chartered Accountants</td>\n      <td>Bristol, England, United Kingdom</td>\n      <td>2024-01-14</td>\n      <td>Bristol</td>\n      <td>United Kingdom</td>\n      <td>Tier</td>\n      <td>Mid senior</td>\n      <td>Onsite</td>\n      <td>Audit, Assurance, Accounting, Business Develop...</td>\n      <td>Description\\nLocation: Bristol\\nAbout The Role...</td>\n      <td>885770</td>\n      <td>4917</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>https://www.linkedin.com/jobs/view/test-engine...</td>\n      <td>2024-01-19 20:05:43.91119+00</td>\n      <td>t</td>\n      <td>t</td>\n      <td>f</td>\n      <td>Test Engineer</td>\n      <td>Stoke Space</td>\n      <td>Moses Lake, WA</td>\n      <td>2024-01-14</td>\n      <td>Ephrata</td>\n      <td>United States</td>\n      <td>Agricultural-Research Engineer</td>\n      <td>Mid senior</td>\n      <td>Onsite</td>\n      <td>Fluid systems, Mechanical systems, Instrumenta...</td>\n      <td>A thriving economy in space is needed to make ...</td>\n      <td>14991</td>\n      <td>4365</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>https://www.linkedin.com/jobs/view/test-engine...</td>\n      <td>2024-01-19 09:45:09.215838+00</td>\n      <td>t</td>\n      <td>t</td>\n      <td>f</td>\n      <td>Test Engineer</td>\n      <td>Akkodis</td>\n      <td>Tempe, AZ</td>\n      <td>2024-01-12</td>\n      <td>Canyon</td>\n      <td>United States</td>\n      <td>Agricultural-Research Engineer</td>\n      <td>Associate</td>\n      <td>Onsite</td>\n      <td>Aerospace, Mechanical test environment, Mechan...</td>\n      <td>Akkodis is seeking a Test Engineer for a posit...</td>\n      <td>189702</td>\n      <td>3999</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>https://www.linkedin.com/jobs/view/test-engine...</td>\n      <td>2024-01-19 09:45:09.215838+00</td>\n      <td>t</td>\n      <td>t</td>\n      <td>f</td>\n      <td>Test Engineer</td>\n      <td>nFocus SolutionsÂ®</td>\n      <td>Orlando, FL</td>\n      <td>2024-01-13</td>\n      <td>Avondale</td>\n      <td>United States</td>\n      <td>Computer Operator</td>\n      <td>Mid senior</td>\n      <td>Onsite</td>\n      <td>Agile Software Development, System Testing, Te...</td>\n      <td>Role Summary\\nWork within an Agile Software De...</td>\n      <td>16993</td>\n      <td>3581</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>https://www.linkedin.com/jobs/view/test-engine...</td>\n      <td>2024-01-19 09:45:09.215838+00</td>\n      <td>t</td>\n      <td>t</td>\n      <td>f</td>\n      <td>Test Engineer</td>\n      <td>Micross Components</td>\n      <td>Los Alamitos, CA</td>\n      <td>2024-01-12</td>\n      <td>Pasadena</td>\n      <td>United States</td>\n      <td>Agricultural-Research Engineer</td>\n      <td>Associate</td>\n      <td>Onsite</td>\n      <td>Semiconductor Testing, Automated Test Equipmen...</td>\n      <td>Job Summary:\\nPerforms LAT testing, builds bur...</td>\n      <td>568816</td>\n      <td>3510</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>https://www.linkedin.com/jobs/view/test-engine...</td>\n      <td>2024-01-19 09:45:09.215838+00</td>\n      <td>t</td>\n      <td>t</td>\n      <td>f</td>\n      <td>Test Engineer</td>\n      <td>SEGULA Technologies</td>\n      <td>Greensboro, NC</td>\n      <td>2024-01-12</td>\n      <td>North Carolina</td>\n      <td>United States</td>\n      <td>Agricultural-Research Engineer</td>\n      <td>Associate</td>\n      <td>Onsite</td>\n      <td>Data acquisition, Data processing, Mechanical ...</td>\n      <td>Company Description\\nMUST be authorized to wor...</td>\n      <td>452512</td>\n      <td>3301</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows Ã 19 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(f\"{path}/data/sampled_linkedin_jd_skills/sampled_data_v2/sampled_jd_resume_set1.csv\"),\n",
    "      pd.read_csv(f\"{path}/data/sampled_linkedin_jd_skills/sampled_data_v2/sampled_jd_resume_set2.csv\")])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing prompts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c3ef317b8959c9d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "i=75"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:32:32.093914Z",
     "start_time": "2025-03-08T23:32:32.092379Z"
    }
   },
   "id": "416793c43b9cb514",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Job role:** Data Engineer\n",
      "\n",
      "**Job Description:** About Outdefine\n",
      "Outdefine is a web3 talent community that connects top talent with leading-edge companies and enterprises globally. Companies choose to hire Outdefine Trusted Members because their skills and readiness have been proven.\n",
      "When you accept a job as a Trusted Member, you will keep all of your pay. Contrast this with traditional hiring networks and agencies that charge membership fees and take up to 50% of your earnings as their markup. Additionally, Trusted Members get access to premier jobs, networking, and a global community powered by tokens. You can earn Outdefine tokens by working, contributing to the community, and referring friends.\n",
      "More than 400 jobs are currently listed on Outdefine, with more being added regularly. Join over 40,000+ professionals from 115 countries who are building and developing their careers with Outdefine.\n",
      "In order to apply for this position, first complete your profile on www.outdefine.com. We want to make sure that your application gets the most attention, so we suggest that you start the assessment process now to become a Trusted Member.\n",
      "To receive direct support from career experts, join Discord.\n",
      "In order to apply for this position\n",
      "ð Signup: https://outdefine.com/r/IgnacioLoyola-0067\n",
      "ð Book a meeting on Assessments' feature\n",
      "Job Overview\n",
      "Location:\n",
      "Onsite, Plano â TX, US\n",
      "8-10 yearsâ experience as Data Engineer\n",
      "Only locals â no relocation\n",
      "Long term contract â 1099 or C2C\n",
      "Visa: USC, GC, H4EAD\n",
      "Required Skills: Spark, Scala, Databricks and AWS\n",
      "Technology: Analytics\n",
      "â¢ Seeking a highly skilled Data Engineer with 8-10 years of experience in. Must have expertise in Spark and Scala.\n",
      "â¢ 5+ Years of IT experience in AWS Cloud, Databricks, AWS S3 , Spark/Scala ,Big Data tool and technologies\n",
      "â¢ Build Data Mapping documents before coding with provided requirements\n",
      "â¢ Excellent knowledge in writing SQL queries for any given problem statement\n",
      "â¢ Programming knowledge in AWS Cloud, AWS Glue ETL, Scala, Java or Python (preferable Scala)\n",
      "â¢ Good understanding of Spark concepts â RDD/Dataframes/Datasets/Streaming and experience with coding in Scala is preferred\n",
      "â¢ Need to have a good understanding of cloud terminologies in AWS and have a good grasp of all basic AWS services (eg. S3/IAM etc.)\n",
      "Responsibilities:\n",
      "â¢ Perform Data Analysis understanding the existing business requirements\n",
      "â¢ Design and build scalable data pipelines using Big Data stack Databricks, Spark Scala, AWS â Athena, S3, Glue ETL, Redshift, Dynamo DB\n",
      "â¢ Design Ingress and Egress patterns for data lake to support self-service use cases and improve time to business value.\n",
      "â¢ Implement CI/CD data pipelines using Git Hub, Jenkins and Artifactory to automate the code build and promotion process.\n",
      "â¢ Engage with customers and principal architects, understand the business requirements and build AWS cloud big data platform and solutions.\n",
      "â¢ Create proof of concepts, evaluate new products and develop a roadmap for NextGen Technology and Digital Cloud First strategy\n",
      "â¢ Jobs Orchestration\n",
      "â¢ Ability to work independently and guide other new team members end-to-end from Development to unit testing to UAT and Prod Deployment/cut over ensuring quality\n",
      "â¢ Any experience with AWS Cloud, AWS Glue ETL, Databricks Delta/Redshift/ Talend Big Data would be added advantage\n",
      "â¢ Good written and oral communication skills\n",
      "â¢ Ability to work independently in agile scrum methodology\n",
      "In order to apply for this position\n",
      "ð Signup: https://outdefine.com/r/IgnacioLoyola-0067\n",
      "ð Book a meeting on Assessments' feature\n",
      "In order to apply for this position, first complete your profile on www.outdefine.com.\n",
      "We want to learn more about you, so we encourage you to provide us with a brief summary of yourself and your past experience as part of the process. As soon as this is completed, you'll take a technical assessment based on your skill set, and if you pass, you'll earn 500 Outdefine tokens. We will review your application, and if you are qualified, we will invite you to a 1:1 video interview.\n",
      "Already a Trusted Member of Outdefine? Then go ahead and apply directly for the job of your dreams.\n",
      "Equal Employment Opportunity\n",
      "We are an equal opportunity employer and do not discriminate against any employee or applicant for employment on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, veteran status, or any other protected status. We are committed to creating a diverse and inclusive environment for all employees and applicants for employment. All qualified individuals are encouraged to apply and will be considered for employment without regard to any legally protected status.\n",
      "Show more\n",
      "Show less\n",
      "\n",
      "**Skills:** Spark, Scala, Databricks, AWS, AWS S3, Big Data, SQL, Cloud terminologies, AWS Glue ETL, Java, Python, RDD, Dataframes, Datasets, Streaming, Git Hub, Jenkins, Artifactory, CI/CD, Agile scrum, Talend Big Data, Unit testing, UAT, Prod Deployment, AWS Cloud, AWS Glue ETL, Databricks Delta, Redshift\n"
     ]
    }
   ],
   "source": [
    "print(\"**Job role:**\", df.iloc[i][\"job_title\"])\n",
    "print(\"\\n**Job Description:**\", df.iloc[i][\"job_summary\"])\n",
    "print(\"\\n**Skills:**\", df.iloc[i][\"job_skills\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:32:32.097278Z",
     "start_time": "2025-03-08T23:32:32.094717Z"
    }
   },
   "id": "87e036dae7706cf9",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating Resume"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64762fdc3f74ed59"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['Complete Mismatch', 'Underwhelming', 'Good Fit', 'Overqualified']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Updated category requirements with enhanced specifications\n",
    "category_requirements = {\n",
    "    \"Complete Mismatch\": \"\"\"\n",
    "The resume is highly professional but entirely unsuitable for the job. There is no meaningful alignment between the candidate's background and the job requirements. The resume must contain 500-700 words in total.\n",
    "\n",
    "- **Career Path:**  \n",
    "  - The candidate has spent their career in an industry that is entirely unrelated to the job.  \n",
    "  - The candidate has worked with technologies completely unrelated to the job.\n",
    "  - Create a logical but irrelevant career progression with 2-3 positions showing growth in the wrong field.\n",
    "\n",
    "- **Skills:**  \n",
    "  - Core Skills: Show expertise in a completely different domain with maximum 10% overlap with required job skills.\n",
    "  - Technical Skills: Focus on technologies from an unrelated field or outdated/irrelevant technologies.\n",
    "  - Additional Skills: Include transferable skills but avoid those specifically mentioned in the job description.\n",
    "\n",
    "- **Work or Project Experience:**\n",
    "  - Generate 3-4 bullet points for each work experience.\n",
    "  - Generate 2-3 bullet points for each project experience.\n",
    "  - Use actual metrics and quantifiable results but in completely unrelated domains.\n",
    "  - Analyze the job description to understand any minimum work experience or skill experience required for the job. Make sure to miss all of those requirements.\n",
    "\n",
    "- **Education & Certifications:**  \n",
    "  - The candidate has degrees that do not contribute to the job requirements.\n",
    "  - The candidate has certifications (if any) that do not contribute to the job requirement.\n",
    "  - Education should be chronologically consistent with work experience.\n",
    "\"\"\",\n",
    "    \n",
    "    \"Underwhelming\": \"\"\"\n",
    "The resume is highly professional but falls noticeably short of the job requirements. The candidate may show potential but lacks key qualifications or experience to perform effectively in the role. The resume must contain 500-600 words in total.\n",
    "\n",
    "- **Career Path:**  \n",
    "  - The candidate has experience in a somewhat related industry but lacks specific job-domain experience.\n",
    "  - The candidate has worked with some related technologies but lacks experience with critical ones.\n",
    "  - Show a career trajectory that suggests interest in transitioning to the job responsibilities, but insufficient specific experience.\n",
    "\n",
    "- **Skills:**  \n",
    "  - Core Skills: Include 2-3 highly relevant skills but miss several critical ones.\n",
    "  - Technical Skills: Show 40-60% overlap with required job skills, with gaps in key areas.\n",
    "  - Additional Skills: Include relevant soft skills that partially compensate for technical gaps.\n",
    "\n",
    "- **Work or Project Experience:**\n",
    "  - Generate 3-4 bullet points for each work experience.\n",
    "  - Generate 2-3 bullet points for each project experience.\n",
    "  - Focus on somewhat relevant but insufficient experiences.\n",
    "  - Include 1-2 projects or responsibilities that demonstrate potential but not proven expertise.\n",
    "  - Analyze the job description to understand any minimum work experience or skill experience required for the job. Make sure to miss some of those requirements.\n",
    "\n",
    "- **Education & Certifications:**  \n",
    "  - The candidate can satisfy the educational requirements.\n",
    "  - The candidate might lack any required certifications.\n",
    "  - Include 1-2 relevant online courses or training to suggest self-improvement efforts.\n",
    "\"\"\",\n",
    "    \n",
    "    \"Good Fit\": \"\"\"\n",
    "The resume is highly professional and is a strong match for the job and meets all key expectations. The candidate is well-qualified and aligns well with the role requirements. The resume must contain 500-600 words in total.\n",
    "\n",
    "- **Career Path:**  \n",
    "  - The candidate has experience in the same or highly related industry as the job requires.\n",
    "  - The candidate has worked with most or all of the technologies mentioned in the job requirements.\n",
    "  - Show a logical progression of roles with increasing responsibility in relevant domains.\n",
    "\n",
    "- **Skills:**  \n",
    "  - Core Skills: Demonstrate 90-100% match with critical job requirements.\n",
    "  - Technical Skills: Show comprehensive coverage of technical requirements with appropriate proficiency levels.\n",
    "  - Additional Skills: Include complementary skills that enhance value (e.g., relevant soft skills, domain knowledge).\n",
    "\n",
    "- **Work or Project Experience:**\n",
    "  - Generate 4-5 bullet points for each work experience.\n",
    "  - Generate 3-4 bullet points for each project experience.\n",
    "  - Include relevant experiences that directly apply to the job requirements.\n",
    "  - For each role, include at least 2 achievements with specific metrics that demonstrate success.\n",
    "  - Analyze the job description to understand any minimum work experience or skill experience required for the job. Make sure to match each of those requirements.\n",
    "  \n",
    "- **Education & Certifications:**  \n",
    "  - The candidate's educational background is an exact match for the job requirements.\n",
    "  - The candidate has relevant certifications that directly contribute to job performance.\n",
    "  - Include any specialized training or continuing education relevant to the role.\n",
    "\"\"\",\n",
    "    \n",
    "    \"Overqualified\": \"\"\"\n",
    "The resume is highly professional and significantly exceeds job requirements, indicating that the candidate may be too advanced for the role. The resume must contain at least 600-800 words in total.\n",
    "\n",
    "- **Career Path:**  \n",
    "  - The candidate has extensive experience in the same industry, exceeding the required experience by 40-100%.\n",
    "  - The candidate has mastered all technologies mentioned in the job requirements plus additional advanced ones.\n",
    "  - Include leadership roles and strategic responsibilities beyond the scope of the target position.\n",
    "\n",
    "- **Skills:**  \n",
    "  - Core Skills: Demonstrate expert-level mastery of all required skills plus additional advanced competencies.\n",
    "  - Technical Skills: Show expertise across broader technology ecosystem relevant to the role.\n",
    "  - Additional Skills: Include leadership, mentoring, and strategic planning abilities beyond the role requirements.\n",
    "\n",
    "- **Work or Project Experience:**\n",
    "  - Generate 5-6 bullet points for each work experience.\n",
    "  - Generate 4-5 bullet points for each project experience.\n",
    "  - Highlight advanced expertise, leadership, and strategic impact.\n",
    "  - Include significant achievements with impressive metrics that exceed typical expectations for the role.\n",
    "  - Document experience mentoring, managing, or leading initiatives beyond the scope of the target role.\n",
    "  - Analyze the job description to understand any minimum work experience or skill experience required for the job. Make sure to exceed each of those requirements by at least 40%.\n",
    "\n",
    "- **Education & Certifications:**  \n",
    "  - The candidate holds degrees beyond what is required for the role (higher level or additional specialized degrees).\n",
    "  - The candidate possesses advanced or numerous certifications beyond what the job requires.\n",
    "  - Include publications, speaking engagements, or industry recognition where relevant.\n",
    "  - Document advanced training or specialized education beyond job requirements.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "categories = ['Complete Mismatch', 'Underwhelming', 'Good Fit', 'Overqualified']\n",
    "categories"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:32:32.102082Z",
     "start_time": "2025-03-08T23:32:32.098075Z"
    }
   },
   "id": "c032543d4671866b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resume_template = \"\"\"\n",
    "You are an expert resume writer with specialized knowledge in talent acquisition and hiring practices at {company}. You are tasked to create a highly professional resume that can be used by Human Resources as a reference to categorize applicants as **{fit_category}**. Your task is to generate a tailored, content-rich resume based on the provided inputs while strictly adhering to the specified fit category constraints. Below are the details of the job that the applicants will be applying to:\n",
    "\n",
    "## **Job Details**:\n",
    "Assume {name} is applying for the company {company}. Below are the details for the job that the candidate will be applying to:\n",
    "- **Role:** {job_role}\n",
    "- **Job Description:** {jd}\n",
    "- **Required Skills:** {skills}\n",
    "\n",
    "## **Fit Category:** {fit_category}\n",
    "## **Category Requirements:**  \n",
    "{category_requirements}\n",
    "\n",
    "## **Resume Format:** {resume_format} # Chronological, Functional, or Hybrid\n",
    "\n",
    "## **Instructions**:\n",
    "1. **Resume Objective**  \n",
    "   - Generate a highly professional resume that precisely aligns with the specified fit category ({fit_category}).\n",
    "   - Ensure the resume maintains industry standards and meets professional expectations.\n",
    "   - Structure the resume to contain clearly defined sections.\n",
    "\n",
    "2. **Candidate Profile**\n",
    "   - **Contact Information**: Generate realistic but fictional phone, email, LinkedIn URL.\n",
    "   - **Professional Summary**: 2-4 sentences highlighting career focus, expertise level, and key strengths aligned with the fit category.\n",
    "\n",
    "3. **Mandatory Sections**  \n",
    "   - **Education**: University name, degree title, major, graduation year, GPA if applicable. For higher education levels, ensure chronological consistency with work experience.\n",
    "   - **Skills**: Comma separated set of skills. Skills must contain \n",
    "       - 5-10 primary skills with highest proficiency\n",
    "       - 10-20 technical competencies with details on proficiency level\n",
    "       - 5-10 complementary abilities, particularly soft skills\n",
    "\n",
    "4. **Optional Sections**  \n",
    "   - **Work Experience**: \n",
    "       - Ignore this section if the job is expecting entry level candidates who are just out of college.\n",
    "       - Generate company names, roles, employment type, duration showing logical career progression.\n",
    "       - Ensure role seniority aligns with experience level and fit category.\n",
    "       - Company sectors should be consistent with career trajectory (avoid random industry jumps unless specified).\n",
    "       - Bullet points must follow APR structure (Action-Project-Result) with appropriate metrics.\n",
    "       - For \"Overqualified\" and \"Good Fit\" categories, show progression in responsibilities.\n",
    "       \n",
    "   - **Projects**: \n",
    "       - Compensate work experience with Project work if the candidate has no prior work experience.\n",
    "       - Include relevant project names (internal, academic, or personal).\n",
    "       - Specify technology stack relevant to the time period of the project.\n",
    "       - For technical roles, include GitHub/portfolio links when appropriate.\n",
    "       - Ensure project complexity scales with fit category.\n",
    "\n",
    "   - **Certifications**: Include industry-relevant certifications with appropriate dates.\n",
    "   - **Publications/Research**: For academic or research-intensive roles where applicable.\n",
    "   - **Professional Associations**: For industry-specific positions.\n",
    "   - **Awards/Recognition**: Scale according to fit category.\n",
    "\n",
    "5. **Bullet Point Constraints**\n",
    "   - A typical resume bullet point format starts with a strong action verb, describes the specific task or project you undertook, and then highlights the quantifiable result or impact you achieved.\n",
    "   - Bullet points follows the \"Action + Project/Problem + Result\" (APR) structure, keeping each bullet point concise and focused on accomplishments.\n",
    "        - *Action Verb*: Begin with a powerful action verb that clearly describes what you did (e.g., \"developed,\" \"managed,\" \"implemented,\" \"analyzed\"). \n",
    "        - *Specific Details*: Briefly explain the project, task, or responsibility you were involved in. \n",
    "        - *Quantifiable Result*: Include numbers, percentages, or other metrics to demonstrate the impact of your work whenever possible. \n",
    "\n",
    "6. **Formatting and Quantification Guidelines**  \n",
    "   - Use **clear section headings**.\n",
    "   - Precede each bullet point with **\"-\"**.\n",
    "   - Mark key entities (**institutions, companies, project names**) with **\"*\"**.\n",
    "   - Each bullet point must be between **150-180 characters**.\n",
    "   - Quantify achievements using metrics appropriate to the role type:\n",
    "       - Engineering: Performance improvements, scale, efficiency gains\n",
    "       - Sales/Marketing: Revenue impact, growth percentages, lead generation\n",
    "       - Management: Team size, budget responsibility, project outcomes\n",
    "       - Operations: Process improvements, cost reductions, time savings\n",
    "\n",
    "7. **Temporal Consistency Requirements**\n",
    "   - Ensure all technologies mentioned align with their actual market availability dates.\n",
    "   - Maintain logical progression of responsibilities and achievements.\n",
    "   - Avoid anachronisms (e.g., claiming experience with technologies before they existed).\n",
    "\n",
    "8. **Industry-Specific Adaptations**\n",
    "   - Adjust terminology density based on role and seniority.\n",
    "   - Include industry-specific metrics and achievements.\n",
    "   - Adapt format slightly based on industry conventions.\n",
    "\n",
    "9. **Output Requirements**  \n",
    "    - Generate **only** the resume; **do not include any explanatory notes or meta-text**.\n",
    "    - Maintain **authenticity, clarity, and professionalism** throughout.\n",
    "    - Total word count must adhere to fit category specifications.\n",
    "    - **DO NOT** mention the fit category in the generated content.\n",
    "    - **DO NOT** include any NOTE at the end of the generated content.\n",
    "    \n",
    "**NOTE**: {name} is an imaginary person who does not exist. Therefore, you would not be violating any data privacy issues.\n",
    "\"\"\"\n",
    "\n",
    "# Additional parameters for enhanced resume generation\n",
    "resume_formats = [\"Chronological\", \"Functional\", \"Hybrid\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:32:32.105642Z",
     "start_time": "2025-03-08T23:32:32.102730Z"
    }
   },
   "id": "a0a0fe42e69a8fc5",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "#Llama 3.2 using Ollama on local system\n",
    "model = OllamaLLM(model=\"llama3.2\", temperature = 0.6)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(resume_template)\n",
    "chain = prompt | model\n",
    "data = {\n",
    "    \"name\": fake.name(),\n",
    "    \"company\": df.iloc[i][\"company\"],\n",
    "    \"job_role\": df.iloc[i][\"job_title\"],\n",
    "    \"jd\": df.iloc[i][\"job_summary\"],\n",
    "    \"skills\":df.iloc[i][\"job_skills\"],\n",
    "    \"fit_category\": categories[2],\n",
    "    \"category_requirements\": category_requirements[categories[2]],\n",
    "    \"resume_format\" : resume_formats[random.choice([0, 1, 2])]\n",
    "}\n",
    "resume1 = chain.invoke(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:33:12.877762Z",
     "start_time": "2025-03-08T23:32:32.106320Z"
    }
   },
   "id": "94cd72ec10287ca8",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'name': 'Daniel Hicks',\n 'company': 'Outdefine',\n 'job_role': 'Data Engineer',\n 'jd': \"About Outdefine\\nOutdefine is a web3 talent community that connects top talent with leading-edge companies and enterprises globally. Companies choose to hire Outdefine Trusted Members because their skills and readiness have been proven.\\nWhen you accept a job as a Trusted Member, you will keep all of your pay. Contrast this with traditional hiring networks and agencies that charge membership fees and take up to 50% of your earnings as their markup. Additionally, Trusted Members get access to premier jobs, networking, and a global community powered by tokens. You can earn Outdefine tokens by working, contributing to the community, and referring friends.\\nMore than 400 jobs are currently listed on Outdefine, with more being added regularly. Join over 40,000+ professionals from 115 countries who are building and developing their careers with Outdefine.\\nIn order to apply for this position, first complete your profile on www.outdefine.com. We want to make sure that your application gets the most attention, so we suggest that you start the assessment process now to become a Trusted Member.\\nTo receive direct support from career experts, join Discord.\\nIn order to apply for this position\\nð Signup: https://outdefine.com/r/IgnacioLoyola-0067\\nð Book a meeting on Assessments' feature\\nJob Overview\\nLocation:\\nOnsite, Plano â TX, US\\n8-10 yearsâ experience as Data Engineer\\nOnly locals â no relocation\\nLong term contract â 1099 or C2C\\nVisa: USC, GC, H4EAD\\nRequired Skills: Spark, Scala, Databricks and AWS\\nTechnology: Analytics\\nâ¢ Seeking a highly skilled Data Engineer with 8-10 years of experience in. Must have expertise in Spark and Scala.\\nâ¢ 5+ Years of IT experience in AWS Cloud, Databricks, AWS S3 , Spark/Scala ,Big Data tool and technologies\\nâ¢ Build Data Mapping documents before coding with provided requirements\\nâ¢ Excellent knowledge in writing SQL queries for any given problem statement\\nâ¢ Programming knowledge in AWS Cloud, AWS Glue ETL, Scala, Java or Python (preferable Scala)\\nâ¢ Good understanding of Spark concepts â RDD/Dataframes/Datasets/Streaming and experience with coding in Scala is preferred\\nâ¢ Need to have a good understanding of cloud terminologies in AWS and have a good grasp of all basic AWS services (eg. S3/IAM etc.)\\nResponsibilities:\\nâ¢ Perform Data Analysis understanding the existing business requirements\\nâ¢ Design and build scalable data pipelines using Big Data stack Databricks, Spark Scala, AWS â Athena, S3, Glue ETL, Redshift, Dynamo DB\\nâ¢ Design Ingress and Egress patterns for data lake to support self-service use cases and improve time to business value.\\nâ¢ Implement CI/CD data pipelines using Git Hub, Jenkins and Artifactory to automate the code build and promotion process.\\nâ¢ Engage with customers and principal architects, understand the business requirements and build AWS cloud big data platform and solutions.\\nâ¢ Create proof of concepts, evaluate new products and develop a roadmap for NextGen Technology and Digital Cloud First strategy\\nâ¢ Jobs Orchestration\\nâ¢ Ability to work independently and guide other new team members end-to-end from Development to unit testing to UAT and Prod Deployment/cut over ensuring quality\\nâ¢ Any experience with AWS Cloud, AWS Glue ETL, Databricks Delta/Redshift/ Talend Big Data would be added advantage\\nâ¢ Good written and oral communication skills\\nâ¢ Ability to work independently in agile scrum methodology\\nIn order to apply for this position\\nð Signup: https://outdefine.com/r/IgnacioLoyola-0067\\nð Book a meeting on Assessments' feature\\nIn order to apply for this position, first complete your profile on www.outdefine.com.\\nWe want to learn more about you, so we encourage you to provide us with a brief summary of yourself and your past experience as part of the process. As soon as this is completed, you'll take a technical assessment based on your skill set, and if you pass, you'll earn 500 Outdefine tokens. We will review your application, and if you are qualified, we will invite you to a 1:1 video interview.\\nAlready a Trusted Member of Outdefine? Then go ahead and apply directly for the job of your dreams.\\nEqual Employment Opportunity\\nWe are an equal opportunity employer and do not discriminate against any employee or applicant for employment on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, veteran status, or any other protected status. We are committed to creating a diverse and inclusive environment for all employees and applicants for employment. All qualified individuals are encouraged to apply and will be considered for employment without regard to any legally protected status.\\nShow more\\nShow less\",\n 'skills': 'Spark, Scala, Databricks, AWS, AWS S3, Big Data, SQL, Cloud terminologies, AWS Glue ETL, Java, Python, RDD, Dataframes, Datasets, Streaming, Git Hub, Jenkins, Artifactory, CI/CD, Agile scrum, Talend Big Data, Unit testing, UAT, Prod Deployment, AWS Cloud, AWS Glue ETL, Databricks Delta, Redshift',\n 'fit_category': 'Good Fit',\n 'category_requirements': \"\\nThe resume is highly professional and is a strong match for the job and meets all key expectations. The candidate is well-qualified and aligns well with the role requirements. The resume must contain 500-600 words in total.\\n\\n- **Career Path:**  \\n  - The candidate has experience in the same or highly related industry as the job requires.\\n  - The candidate has worked with most or all of the technologies mentioned in the job requirements.\\n  - Show a logical progression of roles with increasing responsibility in relevant domains.\\n\\n- **Skills:**  \\n  - Core Skills: Demonstrate 90-100% match with critical job requirements.\\n  - Technical Skills: Show comprehensive coverage of technical requirements with appropriate proficiency levels.\\n  - Additional Skills: Include complementary skills that enhance value (e.g., relevant soft skills, domain knowledge).\\n\\n- **Work or Project Experience:**\\n  - Generate 4-5 bullet points for each work experience.\\n  - Generate 3-4 bullet points for each project experience.\\n  - Include relevant experiences that directly apply to the job requirements.\\n  - For each role, include at least 2 achievements with specific metrics that demonstrate success.\\n  - Analyze the job description to understand any minimum work experience or skill experience required for the job. Make sure to match each of those requirements.\\n  \\n- **Education & Certifications:**  \\n  - The candidate's educational background is an exact match for the job requirements.\\n  - The candidate has relevant certifications that directly contribute to job performance.\\n  - Include any specialized training or continuing education relevant to the role.\\n\",\n 'resume_format': 'Hybrid'}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:33:12.917678Z",
     "start_time": "2025-03-08T23:33:12.895621Z"
    }
   },
   "id": "2dfdcec77f90cc69",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample resume that meets the requirements:\n",
      "\n",
      "**Daniel Hicks**\n",
      "\n",
      "**Contact Information:**\n",
      "- Phone: (123) 456-7890\n",
      "- Email: [dhicks@email.com](mailto:dhicks@email.com)\n",
      "- LinkedIn URL: https://www.linkedin.com/in/danielhickssample/\n",
      "- Address: 123 Main St, Anytown, USA 12345\n",
      "\n",
      "**Professional Summary:**\n",
      "Highly motivated and detail-oriented IT professional with expertise in cloud computing, cybersecurity, and data analytics. Proven track record of delivering high-quality solutions on time and within budget.\n",
      "\n",
      "**Education:**\n",
      "\n",
      "* Bachelor's Degree in Computer Science, XYZ University (2018-2022)\n",
      "\t+ GPA: 3.5/4.0\n",
      "\t+ Relevant Coursework: Cloud Computing, Cybersecurity, Data Structures, Algorithms\n",
      "\n",
      "**Skills:**\n",
      "Primary Skills:\n",
      "- Programming languages: Java, Python, C++\n",
      "- Development frameworks: Spring, Django\n",
      "- Databases: MySQL, PostgreSQL\n",
      "Complementary Abilities:\n",
      "- Communication skills\n",
      "- Team management\n",
      "- Time management\n",
      "\n",
      "Technical Competencies (High Proficiency):\n",
      "- Cloud platforms: AWS, Azure\n",
      "- Cybersecurity frameworks: NIST, OWASP\n",
      "- Data analytics tools: Tableau, Power BI\n",
      "- Agile methodologies: Scrum, Kanban\n",
      "\n",
      "**Projects:**\n",
      "\n",
      "* **Cloud-Based E-commerce Platform**: Developed a cloud-based e-commerce platform using Spring Boot and MongoDB. Implemented a scalable architecture to handle high traffic and ensured data security using AWS IAM.\n",
      "\t+ Technologies Used: Java, Spring Boot, MongoDB, AWS\n",
      "\t+ Achievements: Increased sales by 25% within the first month of launch\n",
      "* **Cybersecurity Awareness Program**: Created an interactive cybersecurity awareness program using Python and HTML. Conducted a pilot run with 50 employees and received positive feedback.\n",
      "\t+ Technologies Used: Python, HTML, CSS\n",
      "\t+ Achievements: Improved employee knowledge of security best practices by 30%\n",
      "\n",
      "**Certifications:**\n",
      "\n",
      "* CompTIA Security+\n",
      "* AWS Certified Cloud Practitioner\n",
      "\n",
      "**Publications/Research:**\n",
      "- Published a research paper on \"Cloud-Based E-commerce Platforms\" in the Journal of Cloud Computing (2022)\n",
      "\n",
      "**Professional Associations:**\n",
      "- Member, International Association for Computer Science and Engineering (IACSIE)\n"
     ]
    }
   ],
   "source": [
    "print(resume1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:33:12.931534Z",
     "start_time": "2025-03-08T23:33:12.927923Z"
    }
   },
   "id": "f0078213be5a75f6",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# #Llama 3.3 with NVIDIA NIM\n",
    "# client = ChatNVIDIA(\n",
    "#   model=\"meta/llama-3.3-70b-instruct\",\n",
    "#   api_key=\"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\", \n",
    "#   temperature=0.6\n",
    "# )\n",
    "# \n",
    "# \n",
    "# prompt = ChatPromptTemplate.from_template(resume_template)\n",
    "# chain = prompt | model\n",
    "# data = {\n",
    "#     \"name\": fake.name(),\n",
    "#     \"company\": df.iloc[i][\"company\"],\n",
    "#     \"job_role\": df.iloc[i][\"job_title\"],\n",
    "#     \"jd\": df.iloc[i][\"job_summary\"],\n",
    "#     \"skills\":df.iloc[i][\"job_skills\"],\n",
    "#     \"fit_category\": categories[2],\n",
    "#     \"category_requirements\": category_requirements[categories[2]],\n",
    "#     \"resume_format\" : resume_formats[random.choice([0, 1, 2])]\n",
    "# }\n",
    "# resume2 = chain.invoke(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:33:12.937281Z",
     "start_time": "2025-03-08T23:33:12.932193Z"
    }
   },
   "id": "2ca1c74d2e76e89d",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print(resume2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:33:12.940972Z",
     "start_time": "2025-03-08T23:33:12.938738Z"
    }
   },
   "id": "df0c6d73d982241c",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#NVIDIA nemotron model listed as a model for synthetic data generation usecase\n",
    "model = ChatNVIDIA(\n",
    "  model=\"nvidia/nemotron-4-340b-instruct\",\n",
    "  api_key=\"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\", \n",
    "  temperature=1.0,\n",
    "        seed = fake.random_number()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:33:12.957195Z",
     "start_time": "2025-03-08T23:33:12.941611Z"
    }
   },
   "id": "6534f27892a34e6d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Good Fit\n",
    "prompt = ChatPromptTemplate.from_template(resume_template)\n",
    "chain = prompt | model\n",
    "data = {\n",
    "    \"name\": fake.name(),\n",
    "    \"company\": df.iloc[i][\"company\"],\n",
    "    \"job_role\": df.iloc[i][\"job_title\"],\n",
    "    \"jd\": df.iloc[i][\"job_summary\"],\n",
    "    \"skills\":df.iloc[i][\"job_skills\"],\n",
    "    \"fit_category\": categories[2],\n",
    "    \"category_requirements\": category_requirements[categories[2]],\n",
    "    \"resume_format\" : resume_formats[random.choice([0, 1, 2])]\n",
    "}\n",
    "resume3 = chain.invoke(data).content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:34:28.171162Z",
     "start_time": "2025-03-08T23:33:12.957858Z"
    }
   },
   "id": "18cd7e3262a199d8",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'name': 'Anthony Solomon',\n 'company': 'Outdefine',\n 'job_role': 'Data Engineer',\n 'jd': \"About Outdefine\\nOutdefine is a web3 talent community that connects top talent with leading-edge companies and enterprises globally. Companies choose to hire Outdefine Trusted Members because their skills and readiness have been proven.\\nWhen you accept a job as a Trusted Member, you will keep all of your pay. Contrast this with traditional hiring networks and agencies that charge membership fees and take up to 50% of your earnings as their markup. Additionally, Trusted Members get access to premier jobs, networking, and a global community powered by tokens. You can earn Outdefine tokens by working, contributing to the community, and referring friends.\\nMore than 400 jobs are currently listed on Outdefine, with more being added regularly. Join over 40,000+ professionals from 115 countries who are building and developing their careers with Outdefine.\\nIn order to apply for this position, first complete your profile on www.outdefine.com. We want to make sure that your application gets the most attention, so we suggest that you start the assessment process now to become a Trusted Member.\\nTo receive direct support from career experts, join Discord.\\nIn order to apply for this position\\nð Signup: https://outdefine.com/r/IgnacioLoyola-0067\\nð Book a meeting on Assessments' feature\\nJob Overview\\nLocation:\\nOnsite, Plano â TX, US\\n8-10 yearsâ experience as Data Engineer\\nOnly locals â no relocation\\nLong term contract â 1099 or C2C\\nVisa: USC, GC, H4EAD\\nRequired Skills: Spark, Scala, Databricks and AWS\\nTechnology: Analytics\\nâ¢ Seeking a highly skilled Data Engineer with 8-10 years of experience in. Must have expertise in Spark and Scala.\\nâ¢ 5+ Years of IT experience in AWS Cloud, Databricks, AWS S3 , Spark/Scala ,Big Data tool and technologies\\nâ¢ Build Data Mapping documents before coding with provided requirements\\nâ¢ Excellent knowledge in writing SQL queries for any given problem statement\\nâ¢ Programming knowledge in AWS Cloud, AWS Glue ETL, Scala, Java or Python (preferable Scala)\\nâ¢ Good understanding of Spark concepts â RDD/Dataframes/Datasets/Streaming and experience with coding in Scala is preferred\\nâ¢ Need to have a good understanding of cloud terminologies in AWS and have a good grasp of all basic AWS services (eg. S3/IAM etc.)\\nResponsibilities:\\nâ¢ Perform Data Analysis understanding the existing business requirements\\nâ¢ Design and build scalable data pipelines using Big Data stack Databricks, Spark Scala, AWS â Athena, S3, Glue ETL, Redshift, Dynamo DB\\nâ¢ Design Ingress and Egress patterns for data lake to support self-service use cases and improve time to business value.\\nâ¢ Implement CI/CD data pipelines using Git Hub, Jenkins and Artifactory to automate the code build and promotion process.\\nâ¢ Engage with customers and principal architects, understand the business requirements and build AWS cloud big data platform and solutions.\\nâ¢ Create proof of concepts, evaluate new products and develop a roadmap for NextGen Technology and Digital Cloud First strategy\\nâ¢ Jobs Orchestration\\nâ¢ Ability to work independently and guide other new team members end-to-end from Development to unit testing to UAT and Prod Deployment/cut over ensuring quality\\nâ¢ Any experience with AWS Cloud, AWS Glue ETL, Databricks Delta/Redshift/ Talend Big Data would be added advantage\\nâ¢ Good written and oral communication skills\\nâ¢ Ability to work independently in agile scrum methodology\\nIn order to apply for this position\\nð Signup: https://outdefine.com/r/IgnacioLoyola-0067\\nð Book a meeting on Assessments' feature\\nIn order to apply for this position, first complete your profile on www.outdefine.com.\\nWe want to learn more about you, so we encourage you to provide us with a brief summary of yourself and your past experience as part of the process. As soon as this is completed, you'll take a technical assessment based on your skill set, and if you pass, you'll earn 500 Outdefine tokens. We will review your application, and if you are qualified, we will invite you to a 1:1 video interview.\\nAlready a Trusted Member of Outdefine? Then go ahead and apply directly for the job of your dreams.\\nEqual Employment Opportunity\\nWe are an equal opportunity employer and do not discriminate against any employee or applicant for employment on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, veteran status, or any other protected status. We are committed to creating a diverse and inclusive environment for all employees and applicants for employment. All qualified individuals are encouraged to apply and will be considered for employment without regard to any legally protected status.\\nShow more\\nShow less\",\n 'skills': 'Spark, Scala, Databricks, AWS, AWS S3, Big Data, SQL, Cloud terminologies, AWS Glue ETL, Java, Python, RDD, Dataframes, Datasets, Streaming, Git Hub, Jenkins, Artifactory, CI/CD, Agile scrum, Talend Big Data, Unit testing, UAT, Prod Deployment, AWS Cloud, AWS Glue ETL, Databricks Delta, Redshift',\n 'fit_category': 'Good Fit',\n 'category_requirements': \"\\nThe resume is highly professional and is a strong match for the job and meets all key expectations. The candidate is well-qualified and aligns well with the role requirements. The resume must contain 500-600 words in total.\\n\\n- **Career Path:**  \\n  - The candidate has experience in the same or highly related industry as the job requires.\\n  - The candidate has worked with most or all of the technologies mentioned in the job requirements.\\n  - Show a logical progression of roles with increasing responsibility in relevant domains.\\n\\n- **Skills:**  \\n  - Core Skills: Demonstrate 90-100% match with critical job requirements.\\n  - Technical Skills: Show comprehensive coverage of technical requirements with appropriate proficiency levels.\\n  - Additional Skills: Include complementary skills that enhance value (e.g., relevant soft skills, domain knowledge).\\n\\n- **Work or Project Experience:**\\n  - Generate 4-5 bullet points for each work experience.\\n  - Generate 3-4 bullet points for each project experience.\\n  - Include relevant experiences that directly apply to the job requirements.\\n  - For each role, include at least 2 achievements with specific metrics that demonstrate success.\\n  - Analyze the job description to understand any minimum work experience or skill experience required for the job. Make sure to match each of those requirements.\\n  \\n- **Education & Certifications:**  \\n  - The candidate's educational background is an exact match for the job requirements.\\n  - The candidate has relevant certifications that directly contribute to job performance.\\n  - Include any specialized training or continuing education relevant to the role.\\n\",\n 'resume_format': 'Hybrid'}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:34:28.181158Z",
     "start_time": "2025-03-08T23:34:28.174325Z"
    }
   },
   "id": "3da881c4903c0d19",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Anthony Solomon**\n",
      "*Data Engineer*\n",
      "\n",
      "*+1 (123) 456-7890* *|* *anthony.solomon@email.com* *|* *linkedin.com/in/anthony-solomon*\n",
      "\n",
      "**Professional Summary**\n",
      "\n",
      "Highly skilled Data Engineer with 10 years of experience in designing and building scalable data pipelines using Big Data stack, AWS, and Databricks. Expertise in Spark, Scala, and AWS S3, with a proven track record of delivering successful projects and driving business value.\n",
      "\n",
      "**Skills**\n",
      "\n",
      "*Primary Skills:* Spark, Scala, Databricks, AWS, AWS S3, Big Data, SQL, Cloud terminologies, AWS Glue ETL, Java, Python, RDD, Dataframes, Datasets, Streaming, Git Hub, Jenkins, Artifactory, CI/CD, Agile scrum, Talend Big Data, Unit testing, UAT, Prod Deployment, AWS Cloud, AWS Glue ETL, Databricks Delta, Redshift\n",
      "\n",
      "*Technical Competencies:* Spark (Expert), Scala (Expert), Databricks (Expert), AWS (Expert), AWS S3 (Expert), Big Data (Advanced), SQL (Advanced), Cloud terminologies (Advanced), AWS Glue ETL (Advanced), Java (Intermediate), Python (Intermediate), RDD (Intermediate), Dataframes (Intermediate), Datasets (Intermediate), Streaming (Intermediate), Git Hub (Intermediate), Jenkins (Intermediate), Artifactory (Intermediate), CI/CD (Intermediate), Agile scrum (Intermediate), Talend Big Data (Intermediate), Unit testing (Intermediate), UAT (Intermediate), Prod Deployment (Intermediate), AWS Cloud (Intermediate), AWS Glue ETL (Intermediate), Databricks Delta (Intermediate), Redshift (Intermediate)\n",
      "\n",
      "*Complementary Abilities:* Problem-solving, Communication, Collaboration, Leadership, Adaptability\n",
      "\n",
      "**Work Experience**\n",
      "\n",
      "*Senior Data Engineer* *|* *Data Solutions Inc.* *|* *Plano, TX* *|* *2018-Present*\n",
      "\n",
      "- Developed and implemented scalable data pipelines using Spark, Scala, and Databricks, reducing processing time by 40%.\n",
      "- Led a team of 5 data engineers in designing and building a data lake on AWS, resulting in a 30% increase in data accessibility.\n",
      "- Optimized data processing using AWS Glue ETL, improving efficiency by 25%.\n",
      "- Collaborated with cross-functional teams to deliver successful projects, ensuring on-time and within-budget completion.\n",
      "\n",
      "*Data Engineer* *|* *Data Analytics Corp.* *|* *Dallas, TX* *|* *2015-2018*\n",
      "\n",
      "- Designed and built data pipelines using Big Data tools and technologies, increasing data processing capacity by 50%.\n",
      "- Implemented CI/CD data pipelines using Git Hub, Jenkins, and Artifactory, reducing deployment time by 30%.\n",
      "- Created proof of concepts and evaluated new products, contributing to the development of a NextGen Technology and Digital Cloud First strategy.\n",
      "- Collaborated with principal architects to understand business requirements and build AWS cloud big data platforms and solutions.\n",
      "\n",
      "**Education**\n",
      "\n",
      "*Master of Science in Computer Science* *|* *University of Texas at Austin* *|* *2013-2015*\n",
      "\n",
      "*Bachelor of Science in Computer Science* *|* *Texas A&M University* *|* *2009-2013*\n",
      "\n",
      "**Certifications**\n",
      "\n",
      "*AWS Certified Data Analytics - Specialty* *|* *2020*\n",
      "\n",
      "*Databricks Certified Associate Developer* *|* *2019*\n",
      "\n",
      "*Certified ScrumMaster (CSM)* *|* *Scrum Alliance* *|* *2018*\n",
      "\n",
      "**Projects**\n",
      "\n",
      "*Real-time Data Processing System* *|* *Data Solutions Inc.* *|* *2020*\n",
      "\n",
      "- Developed a real-time data processing system using Spark Streaming and Kafka, reducing processing time by 60%.\n",
      "- Implemented data visualization using Tableau, enabling real-time data insights for business users.\n",
      "\n",
      "*Data Lake Migration to AWS* *|* *Data Analytics Corp.* *|* *2017*\n",
      "\n",
      "- Led a team in migrating a data lake to AWS, resulting in a 40% reduction in infrastructure costs.\n",
      "- Implemented data security and access control using AWS IAM, ensuring data protection and compliance.\n"
     ]
    }
   ],
   "source": [
    "print(resume3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:34:28.185755Z",
     "start_time": "2025-03-08T23:34:28.182559Z"
    }
   },
   "id": "3bc79b84a645c30c",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can clearly see that the NVIDIA's nemotron model generated the best content. Lets check it for other category fit."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54110c69d1c4326b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Craig Fuller\n",
      "\n",
      "*Email*: craig.fuller@email.com *Phone*: (123) 456-7890 *LinkedIn*: linkedin.com/in/craig-fuller\n",
      "\n",
      "### Professional Summary\n",
      "\n",
      "Highly skilled professional with 12+ years of experience in the automotive industry. Expertise in mechanical engineering, team leadership, and project management. Proven track record in optimizing manufacturing processes and reducing costs.\n",
      "\n",
      "### Skills\n",
      "\n",
      "*Mechanical Engineering, Automotive Design, CAD Software, Manufacturing Processes, Project Management, Team Leadership, Cost Reduction, Process Optimization, Lean Manufacturing, Quality Control, Welding Techniques, Supply Chain Management*\n",
      "\n",
      "### Education\n",
      "\n",
      "*University of Michigan*, Ann Arbor, MI\n",
      "\n",
      "*Bachelor of Science in Mechanical Engineering*, 2008\n",
      "\n",
      "*GPA*: 3.7/4.0\n",
      "\n",
      "### Work Experience\n",
      "\n",
      "*Senior Mechanical Engineer, AutoTech Solutions* (Jun 2018 - Present)\n",
      "\n",
      "- Led a team of 10 engineers in developing and implementing new automotive designs, resulting in a 15% increase in fuel efficiency\n",
      "- Managed a $2M budget for manufacturing process upgrades, reducing downtime by 20% and increasing output by 10%\n",
      "\n",
      "*Mechanical Engineer, Precision Motors* (Jun 2013 - May 2018)\n",
      "\n",
      "- Designed and implemented new welding techniques, reducing production costs by 12% and improving product durability\n",
      "- Managed a team of 5 engineers in optimizing manufacturing processes, leading to a 10% increase in productivity\n",
      "\n",
      "*Junior Mechanical Engineer, MotorWorks* (Jun 2008 - May 2013)\n",
      "\n",
      "- Collaborated with cross-functional teams to develop new automotive components, resulting in a 5% increase in sales\n",
      "- Implemented Lean manufacturing principles, reducing waste by 15% and improving overall efficiency\n",
      "\n",
      "### Projects\n",
      "\n",
      "*Automotive Design Innovation* (2019)\n",
      "\n",
      "- Developed a new automotive design using CAD software, resulting in a 20% reduction in weight and a 10% increase in fuel efficiency\n",
      "\n",
      "*Manufacturing Process Optimization* (2017)\n",
      "\n",
      "- Led a team in optimizing manufacturing processes, reducing production time by 15% and increasing output by 10%\n",
      "\n",
      "### Certifications\n",
      "\n",
      "*Certified Welding Inspector (CWI), American Welding Society (AWS)* (2015)\n",
      "\n",
      "*Project Management Professional (PMP), Project Management Institute (PMI)* (2013)\n",
      "\n",
      "### Awards/Recognition\n",
      "\n",
      "*Engineer of the Year, AutoTech Solutions* (2020)\n",
      "\n",
      "*Outstanding Performance Award, Precision Motors* (2017)\n",
      "\n",
      "*Rising Star Award, MotorWorks* (2011)\n"
     ]
    }
   ],
   "source": [
    "#Complete Mismatch\n",
    "data = {\n",
    "    \"name\": fake.name(),\n",
    "    \"company\": df.iloc[i][\"company\"],\n",
    "    \"job_role\": df.iloc[i][\"job_title\"],\n",
    "    \"jd\": df.iloc[i][\"job_summary\"],\n",
    "    \"skills\":df.iloc[i][\"job_skills\"],\n",
    "    \"fit_category\": categories[0],\n",
    "    \"category_requirements\": category_requirements[categories[0]],\n",
    "    \"resume_format\" : resume_formats[random.choice([0, 1, 2])]\n",
    "}\n",
    "r1 = chain.invoke(data).content\n",
    "print(r1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:35:15.017490Z",
     "start_time": "2025-03-08T23:34:28.186981Z"
    }
   },
   "id": "1ef35ebdff5d6077",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Carlos Mitchell\n",
      "\n",
      "*Plano, TX* | *carlosmitchell@email.com* | *+1 (123) 456-7890* | *linkedin.com/in/carlosmitchell*\n",
      "\n",
      "**PROFESSIONAL SUMMARY**\n",
      "\n",
      "Data Engineer with 6+ years of experience in related industries, demonstrating a strong interest in transitioning to Big Data roles. Proficient in some required technologies, with a focus on continuous learning and development.\n",
      "\n",
      "**SKILLS**\n",
      "\n",
      "*Primary Skills*: Data Engineering, SQL, AWS, Java, Agile Scrum\n",
      "\n",
      "*Technical Competencies*: Spark (proficient), Scala (basic), Databricks (basic), AWS S3, Git Hub, Jenkins, Artifactory, CI/CD, Talend Big Data, Unit testing, UAT, Prod Deployment\n",
      "\n",
      "*Complementary Abilities*: Problem-solving, Communication, Teamwork, Adaptability\n",
      "\n",
      "**WORK EXPERIENCE**\n",
      "\n",
      "*Data Engineer, TechCo*\n",
      "\n",
      "* *Plano, TX*\n",
      "* *March 2019 - Present*\n",
      "\n",
      "- Developed data pipelines using Java and AWS, improving data processing efficiency by 25%.\n",
      "- Collaborated with cross-functional teams to implement CI/CD processes, reducing deployment time by 30%.\n",
      "- Managed data mapping documents and SQL queries for data analysis, ensuring data accuracy and consistency.\n",
      "\n",
      "*Data Analyst, DataAnalytics Inc.*\n",
      "\n",
      "* *Dallas, TX*\n",
      "* *January 2016 - February 2019*\n",
      "\n",
      "- Analyzed and interpreted data to provide insights for business decisions, increasing revenue by 15%.\n",
      "- Created and maintained data reports using SQL and Tableau, ensuring timely delivery of critical information.\n",
      "- Participated in Agile Scrum meetings, contributing to project planning and execution.\n",
      "\n",
      "**PROJECTS**\n",
      "\n",
      "*Personal Project: Big Data Processing*\n",
      "\n",
      "- Implemented a Big Data processing pipeline using Spark and Scala, processing 1 TB of data in 2 hours.\n",
      "- Utilized Databricks for data manipulation and analysis, demonstrating basic proficiency.\n",
      "\n",
      "*Academic Project: Cloud Data Storage*\n",
      "\n",
      "- Designed and implemented a cloud data storage solution using AWS S3, ensuring data security and accessibility.\n",
      "- Collaborated with a team of 4 to develop a proof of concept, showcasing potential in cloud technologies.\n",
      "\n",
      "**EDUCATION**\n",
      "\n",
      "*Bachelor of Science in Computer Science, University of Texas at Austin*\n",
      "\n",
      "* *Austin, TX*\n",
      "* *2012 - 2016*\n",
      "\n",
      "**CERTIFICATIONS**\n",
      "\n",
      "*AWS Certified Developer - Associate*\n",
      "\n",
      "* *2020*\n",
      "\n",
      "**ONLINE COURSES**\n",
      "\n",
      "*Data Engineering with Spark and Scala, Coursera*\n",
      "\n",
      "* *2021*\n",
      "\n",
      "*Big Data Essentials, edX*\n",
      "\n",
      "* *2020*\n"
     ]
    }
   ],
   "source": [
    "#Underwhelming\n",
    "data = {\n",
    "    \"name\": fake.name(),\n",
    "    \"company\": df.iloc[i][\"company\"],\n",
    "    \"job_role\": df.iloc[i][\"job_title\"],\n",
    "    \"jd\": df.iloc[i][\"job_summary\"],\n",
    "    \"skills\":df.iloc[i][\"job_skills\"],\n",
    "    \"fit_category\": categories[1],\n",
    "    \"category_requirements\": category_requirements[categories[1]],\n",
    "    \"resume_format\" : resume_formats[random.choice([0, 1, 2])]\n",
    "}\n",
    "r2 = chain.invoke(data).content\n",
    "print(r2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:36:02.793300Z",
     "start_time": "2025-03-08T23:35:15.019226Z"
    }
   },
   "id": "877f75b277355e57",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Allison Morton\n",
      "\n",
      "*Email*: allison.morton@email.com *Phone*: (123) 456-7890 *LinkedIn*: linkedin.com/in/allison-morton\n",
      "\n",
      "**Professional Summary**\n",
      "\n",
      "Highly skilled and experienced Data Engineer with 12+ years of expertise in designing and implementing scalable data solutions using Spark, Scala, Databricks, and AWS. Demonstrated leadership in managing and mentoring teams, driving strategic initiatives, and delivering high-impact results.\n",
      "\n",
      "**Skills**\n",
      "\n",
      "*Primary Skills*: Spark, Scala, Databricks, AWS, AWS S3, Big Data, SQL, Cloud terminologies, AWS Glue ETL, Java, Python, RDD, Dataframes, Datasets, Streaming, Git Hub, Jenkins, Artifactory, CI/CD, Agile scrum, Talend Big Data, Unit testing, UAT, Prod Deployment, AWS Cloud, AWS Glue ETL, Databricks Delta, Redshift\n",
      "\n",
      "*Technical Competencies*: Expert in Hadoop ecosystem, NoSQL databases, data modeling, data warehousing, and ETL processes. Proficient in containerization technologies (Docker, Kubernetes) and cloud platforms (GCP, Azure).\n",
      "\n",
      "*Complementary Abilities*: Strong problem-solving, communication, and collaboration skills. Proven ability to lead and manage teams, mentor junior engineers, and drive strategic initiatives.\n",
      "\n",
      "**Work Experience**\n",
      "\n",
      "*Senior Data Engineer, Tech Solutions Inc. (2018-Present)*\n",
      "\n",
      "- Led the design and implementation of a Big Data platform using Databricks, Spark Scala, and AWS, resulting in a 60% reduction in data processing time and a 45% decrease in infrastructure costs.\n",
      "- Managed a team of 6 data engineers, providing technical guidance, mentorship, and performance evaluations.\n",
      "- Developed and maintained CI/CD pipelines using Git Hub, Jenkins, and Artifactory, reducing deployment time by 50%.\n",
      "- Collaborated with cross-functional teams to implement data governance policies and ensure data security and compliance.\n",
      "- Optimized data processing pipelines, resulting in a 30% improvement in performance and a 25% reduction in data storage costs.\n",
      "\n",
      "*Data Engineer, Data Innovations (2013-2018)*\n",
      "\n",
      "- Designed and built scalable data pipelines using Big Data stack, resulting in a 50% increase in data processing efficiency and a 35% reduction in data latency.\n",
      "- Implemented data mapping documents and SQL queries for various projects, ensuring data accuracy and consistency.\n",
      "- Collaborated with data scientists to develop predictive models and machine learning algorithms, improving business decision-making and increasing revenue by 15%.\n",
      "- Led the migration of on-premises data infrastructure to AWS, reducing infrastructure costs by 30% and improving data availability and scalability.\n",
      "- Developed and maintained data lake architecture, enabling self-service data access and improving time-to-value by 40%.\n",
      "\n",
      "**Education**\n",
      "\n",
      "*Master of Science in Computer Science, University of Technology (2010-2012)*\n",
      "\n",
      "*Bachelor of Science in Computer Science, University of Technology (2006-2010)*\n",
      "\n",
      "**Certifications**\n",
      "\n",
      "*AWS Certified Solutions Architect - Professional (2020)*\n",
      "\n",
      "*Databricks Certified Associate Developer (2019)*\n",
      "\n",
      "*Cloudera Certified Hadoop Developer (2015)*\n",
      "\n",
      "**Projects**\n",
      "\n",
      "*Real-time Fraud Detection System (2019)*\n",
      "\n",
      "- Developed a real-time fraud detection system using Spark Streaming, Kafka, and Machine Learning algorithms, reducing false positives by 40%.\n",
      "\n",
      "*Data Warehouse Migration to Cloud (2018)*\n",
      "\n",
      "- Led the migration of a 1TB data warehouse from on-premises to AWS Redshift, improving query performance by 50% and reducing infrastructure costs by 30%.\n",
      "\n",
      "**Publications**\n",
      "\n",
      "*Big Data Processing with Spark and Scala: Best Practices and Optimization Techniques (2020)*\n",
      "\n",
      "*Data Lake Architecture for Modern Data Platforms (2019)*\n",
      "\n",
      "*Data Governance and Security in Cloud-based Data Platforms (2018)*\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"name\": fake.name(),\n",
    "    \"company\": df.iloc[i][\"company\"],\n",
    "    \"job_role\": df.iloc[i][\"job_title\"],\n",
    "    \"jd\": df.iloc[i][\"job_summary\"],\n",
    "    \"skills\":df.iloc[i][\"job_skills\"],\n",
    "    \"fit_category\": categories[3],\n",
    "    \"category_requirements\": category_requirements[categories[3]],\n",
    "    \"resume_format\" : resume_formats[random.choice([0, 1, 2])]\n",
    "}\n",
    "r3 = chain.invoke(data).content\n",
    "print(r3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:37:11.593774Z",
     "start_time": "2025-03-08T23:36:02.800444Z"
    }
   },
   "id": "d16d509a43d15baa",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "NVIDIA nemotron seems to be the best model for data generation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2e820054982ddbc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method invoke in module langchain_core.runnables.base:\n",
      "\n",
      "invoke(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output' method of langchain_core.runnables.base.RunnableSequence instance\n",
      "    Transform a single input into an output. Override to implement.\n",
      "\n",
      "    Args:\n",
      "        input: The input to the Runnable.\n",
      "        config: A config to use when invoking the Runnable.\n",
      "           The config supports standard keys like 'tags', 'metadata' for tracing\n",
      "           purposes, 'max_concurrency' for controlling how much work to do\n",
      "           in parallel, and other keys. Please refer to the RunnableConfig\n",
      "           for more details.\n",
      "\n",
      "    Returns:\n",
      "        The output of the Runnable.\n"
     ]
    }
   ],
   "source": [
    "help(chain.invoke)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:37:11.610068Z",
     "start_time": "2025-03-08T23:37:11.600204Z"
    }
   },
   "id": "7525e27074b77981",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RunnableSequence in module langchain_core.runnables.base object:\n",
      "\n",
      "class RunnableSequence(RunnableSerializable)\n",
      " |  RunnableSequence(*steps: 'RunnableLike', name: 'Optional[str]' = None, first: 'Optional[Runnable[Any, Any]]' = None, middle: 'Optional[list[Runnable[Any, Any]]]' = None, last: 'Optional[Runnable[Any, Any]]' = None) -> None\n",
      " |\n",
      " |  Sequence of Runnables, where the output of each is the input of the next.\n",
      " |\n",
      " |  **RunnableSequence** is the most important composition operator in LangChain\n",
      " |  as it is used in virtually every chain.\n",
      " |\n",
      " |  A RunnableSequence can be instantiated directly or more commonly by using the `|`\n",
      " |  operator where either the left or right operands (or both) must be a Runnable.\n",
      " |\n",
      " |  Any RunnableSequence automatically supports sync, async, batch.\n",
      " |\n",
      " |  The default implementations of `batch` and `abatch` utilize threadpools and\n",
      " |  asyncio gather and will be faster than naive invocation of invoke or ainvoke\n",
      " |  for IO bound Runnables.\n",
      " |\n",
      " |  Batching is implemented by invoking the batch method on each component of the\n",
      " |  RunnableSequence in order.\n",
      " |\n",
      " |  A RunnableSequence preserves the streaming properties of its components, so if all\n",
      " |  components of the sequence implement a `transform` method -- which\n",
      " |  is the method that implements the logic to map a streaming input to a streaming\n",
      " |  output -- then the sequence will be able to stream input to output!\n",
      " |\n",
      " |  If any component of the sequence does not implement transform then the\n",
      " |  streaming will only begin after this component is run. If there are\n",
      " |  multiple blocking components, streaming begins after the last one.\n",
      " |\n",
      " |  Please note: RunnableLambdas do not support `transform` by default! So if\n",
      " |      you need to use a RunnableLambdas be careful about where you place them in a\n",
      " |      RunnableSequence (if you need to use the .stream()/.astream() methods).\n",
      " |\n",
      " |      If you need arbitrary logic and need streaming, you can subclass\n",
      " |      Runnable, and implement `transform` for whatever logic you need.\n",
      " |\n",
      " |  Here is a simple example that uses simple functions to illustrate the use of\n",
      " |  RunnableSequence:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def add_one(x: int) -> int:\n",
      " |              return x + 1\n",
      " |\n",
      " |          def mul_two(x: int) -> int:\n",
      " |              return x * 2\n",
      " |\n",
      " |          runnable_1 = RunnableLambda(add_one)\n",
      " |          runnable_2 = RunnableLambda(mul_two)\n",
      " |          sequence = runnable_1 | runnable_2\n",
      " |          # Or equivalently:\n",
      " |          # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |          sequence.invoke(1)\n",
      " |          await sequence.ainvoke(1)\n",
      " |\n",
      " |          sequence.batch([1, 2, 3])\n",
      " |          await sequence.abatch([1, 2, 3])\n",
      " |\n",
      " |  Here's an example that uses streams JSON output generated by an LLM:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          prompt = PromptTemplate.from_template(\n",
      " |              'In JSON format, give me a list of {topic} and their '\n",
      " |              'corresponding names in French, Spanish and in a '\n",
      " |              'Cat Language.'\n",
      " |          )\n",
      " |\n",
      " |          model = ChatOpenAI()\n",
      " |          chain = prompt | model | SimpleJsonOutputParser()\n",
      " |\n",
      " |          async for chunk in chain.astream({'topic': 'colors'}):\n",
      " |              print('-')  # noqa: T201\n",
      " |              print(chunk, sep='', flush=True)  # noqa: T201\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      RunnableSequence\n",
      " |      RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, *steps: 'RunnableLike', name: 'Optional[str]' = None, first: 'Optional[Runnable[Any, Any]]' = None, middle: 'Optional[list[Runnable[Any, Any]]]' = None, last: 'Optional[Runnable[Any, Any]]' = None) -> 'None'\n",
      " |      Create a new RunnableSequence.\n",
      " |\n",
      " |      Args:\n",
      " |          steps: The steps to include in the sequence.\n",
      " |          name: The name of the Runnable. Defaults to None.\n",
      " |          first: The first Runnable in the sequence. Defaults to None.\n",
      " |          middle: The middle Runnables in the sequence. Defaults to None.\n",
      " |          last: The last Runnable in the sequence. Defaults to None.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If the sequence has less than 2 steps.\n",
      " |\n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this Runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  async abatch(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'list[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of outputs from the Runnable.\n",
      " |\n",
      " |  async ainvoke(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Output'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |\n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the Runnable did not implement a native async version of invoke.\n",
      " |\n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |\n",
      " |  async astream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An async iterator of inputs to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  batch(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'list[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Get the graph representation of the Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          config: The config to use. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          The graph representation of the Runnable.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If a Runnable has no first or last node.\n",
      " |\n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'type[BaseModel]'\n",
      " |      Get the input schema of the Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          config: The config to use. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          The input schema of the Runnable.\n",
      " |\n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'type[BaseModel]'\n",
      " |      Get the output schema of the Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          config: The config to use. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output schema of the Runnable.\n",
      " |\n",
      " |  invoke(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  stream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and calls astream.\n",
      " |\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An iterator of inputs to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  get_lc_namespace() -> 'list[str]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |\n",
      " |  is_lc_serializable() -> 'bool' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Check if the object is serializable.\n",
      " |\n",
      " |      Returns:\n",
      " |          True if the object is serializable, False otherwise.\n",
      " |              Defaults to True.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  InputType\n",
      " |      The type of the input to the Runnable.\n",
      " |\n",
      " |  OutputType\n",
      " |      The type of the output of the Runnable.\n",
      " |\n",
      " |  config_specs\n",
      " |      Get the config specs of the Runnable.\n",
      " |\n",
      " |      Returns:\n",
      " |          The config specs of the Runnable.\n",
      " |\n",
      " |  steps\n",
      " |      All the Runnables that make up the sequence in order.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of Runnables.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'first': 'Runnable[Input, Any]', 'last': 'Runnable[...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __parameters__ = ()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'cls': <class 'langchain_core.runnables.ba...\n",
      " |\n",
      " |  __pydantic_custom_init__ = True\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_fields__ = {'first': FieldInfo(annotation=Runnable[TypeVar,...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"RunnableSequence\", val...\n",
      " |\n",
      " |  __signature__ = <Signature (*steps: 'RunnableLike', name: 'Optio...: '...\n",
      " |\n",
      " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'ignore', 'p...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RunnableSerializable:\n",
      " |\n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure alternatives for Runnables that can be set at runtime.\n",
      " |\n",
      " |      Args:\n",
      " |          which: The ConfigurableField instance that will be used to select the\n",
      " |              alternative.\n",
      " |          default_key: The default key to use if no alternative is selected.\n",
      " |              Defaults to \"default\".\n",
      " |          prefix_keys: Whether to prefix the keys with the ConfigurableField id.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: A dictionary of keys to Runnable instances or callables that\n",
      " |              return Runnable instances.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the alternatives configured.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_anthropic import ChatAnthropic\n",
      " |          from langchain_core.runnables.utils import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          model = ChatAnthropic(\n",
      " |              model_name=\"claude-3-sonnet-20240229\"\n",
      " |          ).configurable_alternatives(\n",
      " |              ConfigurableField(id=\"llm\"),\n",
      " |              default_key=\"anthropic\",\n",
      " |              openai=ChatOpenAI()\n",
      " |          )\n",
      " |\n",
      " |          # uses the default model ChatAnthropic\n",
      " |          print(model.invoke(\"which organization created you?\").content)\n",
      " |\n",
      " |          # uses ChatOpenAI\n",
      " |          print(\n",
      " |              model.with_config(\n",
      " |                  configurable={\"llm\": \"openai\"}\n",
      " |              ).invoke(\"which organization created you?\").content\n",
      " |          )\n",
      " |\n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure particular Runnable fields at runtime.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: A dictionary of ConfigurableField instances to configure.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the fields configured.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
      " |              max_tokens=ConfigurableField(\n",
      " |                  id=\"output_token_number\",\n",
      " |                  name=\"Max tokens in the output\",\n",
      " |                  description=\"The maximum number of tokens in the output\",\n",
      " |              )\n",
      " |          )\n",
      " |\n",
      " |          # max_tokens = 20\n",
      " |          print(\n",
      " |              \"max_tokens_20: \",\n",
      " |              model.invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |\n",
      " |          # max_tokens = 200\n",
      " |          print(\"max_tokens_200: \", model.with_config(\n",
      " |              configurable={\"output_token_number\": 200}\n",
      " |              ).invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |\n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the Runnable to JSON.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON-serializable representation of the Runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from RunnableSerializable:\n",
      " |\n",
      " |  __orig_bases__ = (<class 'langchain_core.load.serializable.Serializabl...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __repr_args__(self) -> Any\n",
      " |\n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  lc_id() -> list[str] from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |\n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |      For example, for the class `langchain.llms.openai.OpenAI`, the id is\n",
      " |      [\"langchain\", \"llms\", \"openai\", \"OpenAI\"].\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |\n",
      " |      These attributes must be accepted by the constructor.\n",
      " |      Default is an empty dictionary.\n",
      " |\n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |\n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |\n",
      " |  __repr_name__(self) -> 'str'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      " |      Returns the string representation of a recursive object.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      " |\n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Hook into generating the model's CoreSchema.\n",
      " |\n",
      " |      Args:\n",
      " |          source: The class we are generating a schema for.\n",
      " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      " |\n",
      " |      Returns:\n",
      " |          A `pydantic-core` `CoreSchema`.\n",
      " |\n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_computed_fields\n",
      " |      Get metadata about the computed fields defined on the model.\n",
      " |\n",
      " |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      " |      In V3, this property will be removed from the `BaseModel` class.\n",
      " |\n",
      " |      Returns:\n",
      " |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields\n",
      " |      Get metadata about the fields defined on the model.\n",
      " |\n",
      " |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      " |      In V3, this property will be removed from the `BaseModel` class.\n",
      " |\n",
      " |      Returns:\n",
      " |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Runnable:\n",
      " |\n",
      " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ainvoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details. Defaults to None. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          A tuple of the index of the input and the output from the Runnable.\n",
      " |\n",
      " |  as_tool(self, args_schema: 'Optional[type[BaseModel]]' = None, *, name: 'Optional[str]' = None, description: 'Optional[str]' = None, arg_types: 'Optional[dict[str, type]]' = None) -> 'BaseTool'\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |\n",
      " |      Create a BaseTool from a Runnable.\n",
      " |\n",
      " |      ``as_tool`` will instantiate a BaseTool with a name, description, and\n",
      " |      ``args_schema`` from a Runnable. Where possible, schemas are inferred\n",
      " |      from ``runnable.get_input_schema``. Alternatively (e.g., if the\n",
      " |      Runnable takes a dict as input and the specific dict keys are not typed),\n",
      " |      the schema can be specified directly with ``args_schema``. You can also\n",
      " |      pass ``arg_types`` to just specify the required arguments and their types.\n",
      " |\n",
      " |      Args:\n",
      " |          args_schema: The schema for the tool. Defaults to None.\n",
      " |          name: The name of the tool. Defaults to None.\n",
      " |          description: The description of the tool. Defaults to None.\n",
      " |          arg_types: A dictionary of argument names to types. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A BaseTool instance.\n",
      " |\n",
      " |      Typed dict input:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import List\n",
      " |          from typing_extensions import TypedDict\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          class Args(TypedDict):\n",
      " |              a: int\n",
      " |              b: List[int]\n",
      " |\n",
      " |          def f(x: Args) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      ``dict`` input, specifying schema via ``args_schema``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import Any, Dict, List\n",
      " |          from pydantic import BaseModel, Field\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: Dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          class FSchema(BaseModel):\n",
      " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
      " |\n",
      " |              a: int = Field(..., description=\"Integer\")\n",
      " |              b: List[int] = Field(..., description=\"List of ints\")\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(FSchema)\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      ``dict`` input, specifying schema via ``arg_types``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import Any, Dict, List\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: Dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": List[int]})\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      String input:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: str) -> str:\n",
      " |              return x + \"a\"\n",
      " |\n",
      " |          def g(x: str) -> str:\n",
      " |              return x + \"z\"\n",
      " |\n",
      " |          runnable = RunnableLambda(f) | g\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke(\"b\")\n",
      " |\n",
      " |      .. versionadded:: 0.2.14\n",
      " |\n",
      " |  assign(self, **kwargs: 'Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any], Mapping[str, Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this Runnable.\n",
      " |      Returns a new Runnable.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |          from langchain_core.runnables import Runnable\n",
      " |          from operator import itemgetter\n",
      " |\n",
      " |          prompt = (\n",
      " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |              + \"{question}\"\n",
      " |          )\n",
      " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |\n",
      " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
      " |\n",
      " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
      " |\n",
      " |          print(chain_with_assign.input_schema.model_json_schema())\n",
      " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |          print(chain_with_assign.output_schema.model_json_schema())\n",
      " |          # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |          {'str': {'title': 'Str',\n",
      " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |\n",
      " |  async astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1', 'v2']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      Generate a stream of events.\n",
      " |\n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the Runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |\n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |\n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the Runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the Runnable that emitted the event.\n",
      " |          A child Runnable that gets invoked as part of the execution of a\n",
      " |          parent Runnable is assigned its own unique ID.\n",
      " |      - ``parent_ids``: **List[str]** - The IDs of the parent runnables that\n",
      " |          generated the event. The root Runnable will have an empty list.\n",
      " |          The order of the parent IDs is from the root to the immediate parent.\n",
      " |          Only available for v2 version of the API. The v1 version of the API\n",
      " |          will return an empty list.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the Runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the Runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |\n",
      " |\n",
      " |      Below is a table that illustrates some events that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |\n",
      " |      **ATTENTION** This reference table is for the V2 version of the schema.\n",
      " |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=\"hello world\")           |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | [Document(...), ..]                             |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |\n",
      " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
      " |\n",
      " |      Custom events will be only be surfaced with in the `v2` version of the API!\n",
      " |\n",
      " |      A custom event has following format:\n",
      " |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | Attribute | Type | Description                                                                                               |\n",
      " |      +===========+======+===========================================================================================================+\n",
      " |      | name      | str  | A user defined name for the event.                                                                        |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |\n",
      " |      Here are declarations associated with the standard events shown above:\n",
      " |\n",
      " |      `format_docs`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |\n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |\n",
      " |      `some_tool`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |\n",
      " |      `prompt`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |\n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |\n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      " |          ]\n",
      " |\n",
      " |          # will produce the following events (run_id, and parent_ids\n",
      " |          # has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |\n",
      " |\n",
      " |      Example: Dispatch Custom Event\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.callbacks.manager import (\n",
      " |              adispatch_custom_event,\n",
      " |          )\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
      " |          import asyncio\n",
      " |\n",
      " |\n",
      " |          async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
      " |              \"\"\"Do something that takes a long time.\"\"\"\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 1 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 2 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              return \"Done\"\n",
      " |\n",
      " |          slow_thing = RunnableLambda(slow_thing)\n",
      " |\n",
      " |          async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
      " |              print(event)\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable.\n",
      " |          version: The version of the schema to use either `v2` or `v1`.\n",
      " |                   Users should use `v2`.\n",
      " |                   `v1` is for backwards compatibility and will be deprecated\n",
      " |                   in 0.4.0.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |                   custom events will only be surfaced in `v2`.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |\n",
      " |      Yields:\n",
      " |          An async stream of StreamEvents.\n",
      " |\n",
      " |      Raises:\n",
      " |          NotImplementedError: If the version is not `v1` or `v2`.\n",
      " |\n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a Runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |\n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |\n",
      " |      The Jsonpatch ops can be applied in order to construct state.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable.\n",
      " |          diff: Whether to yield diffs between each step or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          A RunLogPatch or RunLog object.\n",
      " |\n",
      " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[tuple[int, Union[Output, Exception]]]'\n",
      " |      Run invoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |\n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      Useful when a Runnable in a chain requires an argument that is not\n",
      " |      in the output of the previous Runnable or included in the user input.\n",
      " |\n",
      " |      Args:\n",
      " |          kwargs: The arguments to bind to the Runnable.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the arguments bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_community.chat_models import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |\n",
      " |          llm = ChatOllama(model='llama2')\n",
      " |\n",
      " |          # Without bind.\n",
      " |          chain = (\n",
      " |              llm\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |\n",
      " |          # With bind.\n",
      " |          chain = (\n",
      " |              llm.bind(stop=[\"three\"])\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |\n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'type[BaseModel]'\n",
      " |      The type of config this Runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |\n",
      " |  get_config_jsonschema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the config of the Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the config of the Runnable.\n",
      " |\n",
      " |      .. versionadded:: 0.3.0\n",
      " |\n",
      " |  get_input_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the input to the Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the input to the Runnable.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |\n",
      " |              runnable = RunnableLambda(add_one)\n",
      " |\n",
      " |              print(runnable.get_input_jsonschema())\n",
      " |\n",
      " |      .. versionadded:: 0.3.0\n",
      " |\n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the Runnable.\n",
      " |\n",
      " |  get_output_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the output of the Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the output of the Runnable.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |\n",
      " |              runnable = RunnableLambda(add_one)\n",
      " |\n",
      " |              print(runnable.get_output_jsonschema())\n",
      " |\n",
      " |      .. versionadded:: 0.3.0\n",
      " |\n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'list[BasePromptTemplate]'\n",
      " |      Return a list of prompts used by this Runnable.\n",
      " |\n",
      " |  map(self) -> 'Runnable[list[Input], list[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that maps a list of inputs to a list of outputs.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |                  from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |                  def _lambda(x: int) -> int:\n",
      " |                      return x + 1\n",
      " |\n",
      " |                  runnable = RunnableLambda(_lambda)\n",
      " |                  print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n",
      " |\n",
      " |  pick(self, keys: 'Union[str, list[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the output dict of this Runnable.\n",
      " |\n",
      " |      Pick single key:\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              import json\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              chain = RunnableMap(str=as_str, json=as_json)\n",
      " |\n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |\n",
      " |              json_only_chain = chain.pick(\"json\")\n",
      " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> [1, 2, 3]\n",
      " |\n",
      " |      Pick list of keys:\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Any\n",
      " |\n",
      " |              import json\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              def as_bytes(x: Any) -> bytes:\n",
      " |                  return bytes(x, \"utf-8\")\n",
      " |\n",
      " |              chain = RunnableMap(\n",
      " |                  str=as_str,\n",
      " |                  json=as_json,\n",
      " |                  bytes=RunnableLambda(as_bytes)\n",
      " |              )\n",
      " |\n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |\n",
      " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |\n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n",
      " |\n",
      " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
      " |\n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |\n",
      " |              def mul_two(x: int) -> int:\n",
      " |                  return x * 2\n",
      " |\n",
      " |              runnable_1 = RunnableLambda(add_one)\n",
      " |              runnable_2 = RunnableLambda(mul_two)\n",
      " |              sequence = runnable_1.pipe(runnable_2)\n",
      " |              # Or equivalently:\n",
      " |              # sequence = runnable_1 | runnable_2\n",
      " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |              sequence.invoke(1)\n",
      " |              await sequence.ainvoke(1)\n",
      " |              # -> 4\n",
      " |\n",
      " |              sequence.batch([1, 2, 3])\n",
      " |              await sequence.abatch([1, 2, 3])\n",
      " |              # -> [4, 6, 8]\n",
      " |\n",
      " |  with_alisteners(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      on_start: Asynchronously called before the Runnable starts running.\n",
      " |      on_end: Asynchronously called after the Runnable finishes running.\n",
      " |      on_error: Asynchronously called if the Runnable throws an error.\n",
      " |\n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Asynchronously called before the Runnable starts running.\n",
      " |              Defaults to None.\n",
      " |          on_end: Asynchronously called after the Runnable finishes running.\n",
      " |              Defaults to None.\n",
      " |          on_error: Asynchronously called if the Runnable throws an error.\n",
      " |              Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          import time\n",
      " |\n",
      " |          async def test_runnable(time_to_sleep : int):\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(time_to_sleep)\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          async def fn_start(run_obj : Runnable):\n",
      " |              print(f\"on start callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(3)\n",
      " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          async def fn_end(run_obj : Runnable):\n",
      " |              print(f\"on end callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(2)\n",
      " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          async def concurrent_runs():\n",
      " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      " |\n",
      " |          asyncio.run(concurrent_runs())\n",
      " |          Result:\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637053+00:00\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637150+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638305+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638383+00:00\n",
      " |          Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\n",
      " |          Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\n",
      " |          Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:35.640534+00:00\n",
      " |          Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:37.640574+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:37.640654+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:39.641751+00:00\n",
      " |\n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          config: The config to bind to the Runnable.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the config bound.\n",
      " |\n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      The new Runnable will try the original Runnable, and then each fallback\n",
      " |      in order, upon failures.\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original Runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |              Defaults to (Exception,).\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      " |              and its fallbacks must accept a dictionary as input. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original Runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Iterator\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableGenerator\n",
      " |\n",
      " |\n",
      " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |                  raise ValueError()\n",
      " |                  yield \"\"\n",
      " |\n",
      " |\n",
      " |              def _generate(input: Iterator) -> Iterator[str]:\n",
      " |                  yield from \"foo bar\"\n",
      " |\n",
      " |\n",
      " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |                  [RunnableGenerator(_generate)]\n",
      " |                  )\n",
      " |              print(''.join(runnable.stream({}))) #foo bar\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original Runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original Runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |  with_listeners(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      on_start: Called before the Runnable starts running, with the Run object.\n",
      " |      on_end: Called after the Runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the Runnable throws an error, with the Run object.\n",
      " |\n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Called before the Runnable starts running. Defaults to None.\n",
      " |          on_end: Called after the Runnable finishes running. Defaults to None.\n",
      " |          on_error: Called if the Runnable throws an error. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          from langchain_core.tracers.schemas import Run\n",
      " |\n",
      " |          import time\n",
      " |\n",
      " |          def test_runnable(time_to_sleep : int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |\n",
      " |          def fn_start(run_obj: Run):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |\n",
      " |          def fn_end(run_obj: Run):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |\n",
      " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          chain.invoke(2)\n",
      " |\n",
      " |  with_retry(self, *, retry_if_exception_type: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
      " |              Defaults to (Exception,).\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
      " |              time between retries. Defaults to True.\n",
      " |          stop_after_attempt: The maximum number of attempts to make before\n",
      " |              giving up. Defaults to 3.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          count = 0\n",
      " |\n",
      " |\n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                   pass\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |\n",
      " |          assert (count == 2)\n",
      " |\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |  with_types(self, *, input_type: 'Optional[type[Input]]' = None, output_type: 'Optional[type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          input_type: The input type to bind to the Runnable. Defaults to None.\n",
      " |          output_type: The output type to bind to the Runnable. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the types bound.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from Runnable:\n",
      " |\n",
      " |  input_schema\n",
      " |      The type of input this Runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |  output_schema\n",
      " |      The type of output this Runnable produces specified as a pydantic model.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __init_subclass__(...) from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Function to initialize subclasses.\n"
     ]
    }
   ],
   "source": [
    "help(chain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:37:11.648520Z",
     "start_time": "2025-03-08T23:37:11.611582Z"
    }
   },
   "id": "26345b1e41a4133b",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T23:37:11.652481Z",
     "start_time": "2025-03-08T23:37:11.650237Z"
    }
   },
   "id": "6127d5a02e5edd80",
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ars",
   "language": "python",
   "display_name": "Python (ARS)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
